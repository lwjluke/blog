---
date: 2025-05-09 18:27:16
layout: post
title: 理解深度学习与神经网络
---

# 理解深度学习与神经网络

## 一、机器学习的概念

### 人类学习

**学习**是指有智能的生物通过与外部环境的交互，从而获得对外部环境的认识，以及习得如何针对外部环境的输入作出反馈及改变的能力。

学习分为两部分：一是**知识**，就是获取外部世界的认识，并将外部世界的现象在人类大脑中**抽象模型**，此后人就可以通过此知识模型模拟现实世界。

二是**技能**或行为，就是具备输出及改变外部环境的能力，核心就是对己学习到的知识的具体应用，以达到适应环境或改变环境的能力。

总结起来，学习是智能个体与外部环境的交互，分为知（认识）与行（互动）两个主要部分，中国古人有句著名话叫“知行合一"，就表明了学习的这两个方面。

### 机器学习

一般认为有智能的生物才具备学习能力。机器学习是使用计算机实现学习能力，其中认识的部分就是外部环境（即数据中）找到逻辑规律或结构模式，而技能的部分就是应用此学习到的模式或规律，实现对其它输入数据的处理并输出。

传统的机器是没有学习能力的，核心的部分在于机器只有执行的部分，而没有认知的部分，所以机器是“机械的"。机器的知识是人类将自己认知的知识提炼出实作步骤，然后用程序的方式告知机器（这里的程序可以是现代的编程，也可以是传统的物理机械组合），才后机器按照人类输入的知识严格执行，以实现具体的功能。

然而知识的提取与编码工作是十分复杂的，这就是程序员的工作。是否可以将知识的获取也由机器才做，这是人工智能及机器学习的核心问题。

机器学习的核心就是如何实现学习，即从外部数据中提取逻辑规律或抽象模型，此模型能应用于同类场景或同类数据。

### 学习原理

外部的信息通过各种输入信道输入到大脑，学习的核心原理是对输入信息的**整合**、**理解**、**记忆** 、**生成。**

- 整合
  - 不同信道传入的信息需要通过一定方式整合为一个整体的信息
  - 不同时间先后传入的信息也需要通过一定的方式整合为一个整体的信息
  - 理解的基础是整合不同时间、不同信道的所有信息后整合成为的综合信息

* 理解
  * 学习的最核心过程是对知识的理解
  * 理解从过程层面讲可以认为是对输入信息在大脑内的**建立模型**，形成结构化的知识
  * 理解从功能层面讲是对一个输入信息的本质与内涵的解析与获取，将信息转换为意义
  * 理解可以认为是输入信息数据的函数映射，映射后的结果即为知识
* 记忆
  * 记忆是将信息理解后，转换成为的知识的存储
  * 大脑由数百亿带状态的简单计算单元即神经元组成，记忆由神经网络完成
  * 记忆的存储是建立或调整神经元连结以及神经元状态
* 生成
  * 当知识需要输出时，会有一种触发机制触发输出
  * 记忆的输出是以触发信息为基础作为输出，由神经元计算重新产生的输出
  * 输出是通过神经元计算重新生成，而不是原样的提取

学习的过程是对信息的加工及转换：

$$
外部信息 \longrightarrow 结构化知识（意义空间）\longrightarrow 输出知识
$$

### 建模及拟合

机器学习的核心是如何从数据中学习到规律或模式，形成一个特定的模型，然后此模型代表一类知识，可以应用于其它未知数据。

这里先有一个基础假设：数据是有意义，也是内在蕴含着规律和模式的：因为数据是由现实的某一系统生成的，这个系统可以看作一个现实模型，由这个现实模型产生的数据会一定程序体现这个现实模型的结构。

我们需要由机器通过数据通过归纳的方式重构一个模型，此模型与现实系统类似，能够针对同样的输入得到同样或类似的输出。

把这个模型当作一个黑盒，只关心输入及输出时，我们可以将这个模型看作是一个复杂的数学函数：

$$
(y_1,y_2,...,y_n)=f(x_1,x_2,...,x_n)
$$

把输入及输出数据写成**向量**的方式，简化上式为：

$$
Y=f(X)
$$

机器学习的目的就是如何给定的从输入及输出数据$(X,Y)$中，找到代表模型的函数$f(...)$的过程。

当找到代表模型的函数$f(...)$后，我们就可以运行此函数，输入一组新的$X_1$而得到新的输出$Y_1$

在机器学习中，学习的过程即根据数据找到模型函数的过程叫**训练**，应用此模型函数的过程叫**推理或预测**。

### 机器学习模型

由上所述，机器学习可以抽象成为数学上的函数$Y=f(X)$：

$f$：机器学习需要学习的函数模型或者叫广义函数，可以是数学上的函数，也可以是一个带状态的系统。

$X$：输入数据（也就是外部环境数据），如可以各种类型如数字、文本、语音、图像等类型的数据。

$Y$：针对输入数据的广义输出数据，同样可以是多类型的数据。

对于学习到的**函数模型**，需要有一种评估来确认是否此模型能比较合地拟合

这里的输出$Y$可以是具体的输出数据，那么通过$(X,Y)$求$f(...)$的过程叫“有标签学习或监督学习"，

如果输出$Y$是根据一组规则或程序生成，不是具体的数据，那么求$f(...)$的过程叫”无监督学习"

如果输出$Y$是一组可能输出集合，再加上学习过程中的对特定输出的动态反馈评估，那么求$f(...)$的过程叫“”强化学习"（对某种输出作肯定强化，类似基于行为主义的儿童教育）

| 学习类型   | 标签数据$Y$  | 核心目标                 | 典型应用               |
| ---------- | -------------- | ------------------------ | ---------------------- |
| 监督学习   | 有标签$Y$    | 学习输入与标签的映射关系 | 分类、回归             |
| 无监督学习 | 由算法规则替代 | 发现数据内在结构         | 聚类、降维             |
| 自监督学习 | 无标签         | 通过预训练任务生成伪标签 | NLP、计算机视觉        |
| 半监督学习 | 少量标签       | 利用无标签数据辅助训练   | 医疗影像分析、语音识别 |
| 强化学习   | 无标签         | 通过试错和奖励优化策略   | 游戏AI、机器人控制     |

## 二、深度学习与神经网络

### 基础模型

###### 数学模型

机器学习模型可以看作是数学模型，有二类模型：一是曲线拟合，二是逻辑解析

- 广义拟合（输入/输出拟合）：

机器学习模型为广义上的函数：$Y=f(X)$，数学上函数可以在几何空间上表示为一条曲线（二维情况是一条曲线，多维下表示为一个m维曲面），模型$f(...)$的求解其实就是找出一条复杂的曲线，此曲线能拟合尽可能多的数据点$(X,Y)$。

曲线拟合是机器学习的核心原理，回归算法、SVM、神经网络等机器学习的算法都是曲线拟合原理。

- 逻辑解析

逻辑解析是根据所有输入数据的属性类型进行统计后作概率解析而得到结果，有具体而清析的内部逻辑过程，以及过程层面的确定性，这是机器学习中的另一类学习方法，决策树、朴素贝叶斯等都是属于逻辑解析类的方法。

###### 控制系统模型

![1744682813907](image/1.机器学习与神经网络/1744682813907.png)

机器学习也可以看成是一个带有反馈的控制系统，此系统由如下部分组成：

- 功能系统（模型函数）：$Y=f(X)$
- 评估系统：用来评估输出是否达合符预定的评价指标，通常由损失函数(误差函数)定义
- 反馈调节系统：根据评估器的指标反馈调节模型参数，使模型函数的参数更优

机器学习的目标是找到评价最好的拟合模型，也就是损失函数最小或误差最小的模型函数。

学习参数与超参数：

- 学习参数是由输入输出数据在拟合过程中调整的参数，即由输入输出数据决定的拟合参数
- 超参数是由开发者指定的与模型函数有关的经验参数，不会受到输入输出的数据的影响

机器学习的拟合函数参数求解方法：

- 解析方法：根据模型函数与损失函数，使用数学上在公式推导求解，这种方法只适用于简单的回归模型
- 数值迭代方法：由于模型函数太复杂，不能求出解析解。或者由于解析解计算有约束条件不能满足，因而使用数值方法求解模型函数的学习参数。

###### 常规拟合函数

对于广义拟合函数，常用的拟合函数有：

- 线性回归：$y=a_0+a_1 x_1+a_2 x_2 + \cdots + a_n x_n$
- 多项式回归：$y=a_0+a_1 x^1+a_2 x^2 + \cdots + a_n x^n$
- 逻辑回归：$P(y=1)=\sigma(z) = \frac{1}{1 + e^{-z}} \ ,\ z=a_0+a_1 x^1+a_2 x^2 + \cdots + a_n x^n$

### 感知机

##### 大脑结构原理

大脑是由数百亿神经元组成，这些神经元相互连接成网络，构成人类神经网络形成人类的学习功能。用抽象的原理讲，神经网络可以完成人类的任何信息处理功能，完成人类各种复杂的学习任务。

如果人类大脑中的神经元网络作看是一个学习模型或者学习函数，那么人类神经网络可以看成是一个通用的模型或通用的拟合函数。

##### 神经元

人脑中的神经元形状可以用下图做简单的说明：

![1746059547106](image/1.机器学习与神经网络/1746059547106.png "人类神经元结构图")

其中：

- 树突：一个神经元有多个树突，是神经元的输入，这些权突接收来自其它神经元的输出，每个树突对输入信息有各自的效率权重
- 细胞体：负责整合所有加权后的树突信息作为一个综合电位，当综合电位达到某一设定的阈值时，会激活细胞体传出信号
- 轴突：输出输号的传输通道
- 轴突末梢：输出信号与其它神经元的树突的连接

##### [单层感知机](https://www.cnblogs.com/subconscious/p/5058741.html)

基于神经元细胞的结构特性与传递信息方式，神经科学家 Warren McCulloch 和逻辑学家 Walter Pitts 合作提出了“McCulloch–Pitts (MCP) neuron”模型。在人工神经网络中，MCP模型成为人工神经网络中的最基本结构。MCP模型结构如下图所示。

![1746063308439](image/1.机器学习与神经网络/1746063308439.png)

其中输入参数（模拟树突)：

- $x_1,x_2,\cdots,x_n$为神经元的输入信号
- $w_1,w_2,\cdots,w_n$为每个神经元的权重值
- $b$为神经元的阈值

计算函数（模拟细胞体功能)：

- 求和函数：$\mathrm{SUM}=\sum^{n-1}_{i=0}{x_i w_i}+b=\sum^n_{i=0}{x_i w_i}$
- 激活函数：非线的函数$\Phi$

1957年 Frank Rosenblatt 提出了一种简单的人工神经网络，被称之为感知机。早期的感知机结构和 MCP 模型相似，由一个输入层和一个输出层构成，因此也被称为“单层感知机”。感知机的输入层负责接收实数值的输入向量，输出层则为1或-1两个值。单层感知机可作为一种二分类线性分类模型:

![1746063523206](image/1.机器学习与神经网络/1746063523206.png)

单层感知机可被用来区分线性可分数据。在 **图2** 中，逻辑与(AND)、逻辑与非(NAND)和逻辑或(OR)为线性可分函数，所以可利用单层感知机来模拟这些逻辑函数。但是，由于逻辑异或（XOR）是非线性可分的逻辑函数，因此 **单层感知机无法模拟逻辑异或函数的功能** 。

![1746063573421](image/1.机器学习与神经网络/1746063573421.png)

##### 多层感知机

多层感知机由输入层、输出层和至少一层的隐藏层构成。网络中各个隐藏层中神经元可接收相邻前序隐藏层中所有神经元传递而来的信息，经过加工处理后将信息输出给相邻后续隐藏层中所有神经元。

![1744704008900](image/1.机器学习与神经网络/1744704008900.png)

在多层感知机中，相邻层所包含的神经元之间通常使用“全连接”方式进行连接。所谓“全连接”是指两个相邻层之间的神经元相互成对连接，但同一层内神经元之间没有连接。多层感知机可以模拟复杂非线性函数功能，所模拟函数的复杂性取决于网络隐藏层数目和各层中神经元数目。

理论证明，**多层神经网络可以无限逼近任意连续函数，即多层神经网络可以拟合任意函数**，与人类大脑一样，可以学习任何知识。

多层感知机(Multilayer Perceptron, MLP)也称为全连接神经网络(Fully Connected Neural Network, FCNN)，或前馈神经网络(Feedforward Neural Network, FFNN)。

##### 深度神经网络

深度神经网络（DNN）是一种具有多个隐藏层的人工神经网络，通过多层非线性变换从原始数据中逐步提取复杂特征，用于解决图像识别、自然语言处理等复杂任务。

深度神经网络一般分为两个大的部分：

- **特征处理部分**：特征处理从原始数据（如图像像素、文本单词）中提取低级到高级的特征表示，为后续的建模（决策或预测）提供更有信息量的输入。特征处理（Feature Extraction）通常由专门设计的组件或层完成，这些组件负责从原始数据中提取有意义的特征，针对不同的数据类型或应用类型，有不同的特征处理方案：
  - 针对层次模型如图像特征处理的卷积层、池化层
  - 针对序列模型如语音特征处理的循环层（**RNN/LSTM/GRU**）
  - 针对离散模型如文本特征处理的嵌入层、注意力机制
  - 其它通用的特征处理如归一化层、残差层等
- **建立模型或函数拟合部分**：使用处理好的特征数据与输出数据进行建模或函数拟合，这是深度神经网络的核心部门

以下是常见的神经网络类型（CNN、RNN、Transformer、GAN 等）在特征处理和建模部分的组件及功能的对比表格：

| 神经网络类型                  | 特征处理部分（Feature Extraction）                                       | 建模/决策部分（Modeling/Decision）         | 主要应用场景                     |
| ----------------------------- | ------------------------------------------------------------------------ | ------------------------------------------ | -------------------------------- |
| **多层感知机MLP**       | 无专门特征处理组件                                                       | 全连接层（全连接层完成分类或回归）         | 简单分类、回归任务               |
| **卷积神经网络CNN**     | 卷积层（提取局部特征，如边缘、纹理）、<br />池化层（降维、增强鲁棒性）   | 全连接层（将特征映射为分类概率或回归值）   | 图像分类、目标检测、图像生成     |
| **循环神经网络RNN**     | 循环层（捕捉序列数据的时序依赖，<br />如 LSTM/GRU 处理上下文）           | 全连接层（输出分类结果、预测值或序列标签） | 时间序列预测、语音识别、文本生成 |
| **Transformer**         | 嵌入层（将输入转为向量）、<br />注意力机制（聚焦重要信息）               | 全连接层及解码器（生成文本、分类结果）     | 机器翻译、文本生成（NLP 任务）   |
| **生成对抗网络GAN**     | 生成器（提取潜在特征，生成伪数据）、<br />判别器的卷积层（提取判别特征） | 判别器的全连接层（判断真伪，输出概率）     | 图像生成、数据增强、风格迁移     |
| **自编码器Autoencoder** | 编码器（提取压缩特征表示，降维）                                         | 解码器（重建输入数据，常为镜像结构）       | 降维、去噪、特征学习             |

 **说明** ：

* **特征处理部分** ：负责从原始数据中提取有意义的特征，通常是网络的前半部分，使用专门设计的层或机制（如卷积、注意力）。
* **建模/决策部分** ：负责将提取的特征映射到目标输出，通常是网络的后半部分，常使用全连接层或其他映射机制。
* 不同神经网络根据任务需求设计特征处理组件，MLP 由于缺乏专门的特征提取机制，在复杂任务中效率较低，常作为其他网络的决策部分或用于简单任务。

### 深度神经网络结构

##### 神经网络函数

深度神经网络是一输入流在多层级的复杂数据处理网络(包括特征处理函数及神经网络函数)中，经过各个结点或子函数的处理后得到输出结果。

神经网络从函数观点看，就是线性函数与非线线函数的复杂组合，这样组合而成的复合函数是可以摸拟任何函数的通用函数：

- **线性函数**：线性函数提供最基本的线段单元

  * 线性函数在神经网络中通常表现为加权和操作，数学形式为$z=Wx+b$，其中 $W$是权重矩阵，$x$ 是输入，$b$偏置。这是一种线性变换，输出是输入的线性组合。
  * 线性函数提供了最基本的“线段单元”，可以将输入空间划分为不同的线性区域，或者对输入进行简单的缩放、平移和组合。
  * 线性函数本身是有限的，无法模拟非线性关系。如果神经网络仅由线性函数组成（不含非线性激活），无论有多少层，其整体效果仍然等价于单层线性函数，无法解决复杂问题（如 XOR 问题）。
- 非线性**激活函数**：激活函数对基本的线段单元进行分段组合

  - 非线性激活函数（如 ReLU、sigmoid、tanh）在每一层线性变换后引入非线性，数学形式为 $y=f(z)$ 是激活函数，$z$ 是线性变换的输出。
  - 激活函数对线性函数生成的“线段单元”进行分段组合，打破线性限制，使得神经网络能够模拟复杂的非线性关系。使得网络可以构建分段线性函数，从而逼近任意形状的函数。
- **复合函数**：线性函数与非线线函数的复杂组合组成的复合函数就是神经网络，是可以摸拟任何函数的通用函数

  - 经网络通过线性函数（加权和）和非线性激活函数的交替堆叠，形成一个复杂的复合函数。第$k$层可以表示为：

    $$
    y^{(k)}=\phi^{(k)}(W^{(k)}y^{(k-1)}+b^{(k)})
    $$
  - 这种复合函数通过多层嵌套，将输入空间分割为无数小的区域，并在每个区域内应用不同的线性变换，从而实现对复杂函数的逼近

    - 线性函数定义了直线或超平面，分割输入空间
    - 非线性激活函数对这些分割进行“扭曲”或“折叠”，形成复杂的非线性决策边界。
    - 随着层数增加，网络能够构建越来越精细的分割，形成高度复杂的函数形状。

##### **通用逼近定理**

以上使用线性函数与非线性激动函数的深度组合的复合函数，是可以拟合任意函数的通用函数，这个理论就是**通用逼近定理Universal Approximation Theorem**，通用逼近定理是深度神经网络的数学理论基础。

一个具有一个隐藏层的神经网络可以表示为以下形式：

$$
F(x) = \sum_{i=1}^{k} v_i \cdot \phi(w_i^T x + b_i)
$$

其中：

- $k$ 是隐藏层神经元数量。
- $w_i \in \mathbb{R}^n$ 是第 $i$ 个神经元的输入权重向量。
- $b_i \in \mathbb{R}$ 是第 $i$ 个神经元的偏置。
- $\phi: \mathbb{R} \to \mathbb{R}$ 是激活函数，通常是非线性且连续的，如 sigmoid 函数 $\phi(z) = \frac{1}{1 + e^{-z}}$ 或 tanh 函数。
- $v_i \in \mathbb{R}^m$ 是第 $i$ 个隐藏神经元到输出层的权重（对于多输出情况）。
- $w_i^T x + b_i$ 是线性变换，$\phi(w_i^T x + b_i)$ 是非线性变换后的隐藏层输出。

通用逼近定理表明，

- 通过选择足够多的隐藏神经元 $k$ 和适当的参数 $w_i, b_i, v_i$，$N(x)$ 可以任意接近目标函数 $f(x)$。
- 线性函数与非线线函数的复杂组合组成的复合函数，是可以摸拟任何函数的通用函数。
- 或者说一个具有至少一个隐藏层、足够多神经元、适当激活函数的神经网络，可以以任意精度逼近任何连续函数（在紧凑集上）。

##### 神经网络组成

特征处理模块（如卷积层、循环层）和建模部分（如全连接层）在深度神经网络中的函数模块通常基于线性变换（如矩阵乘法或卷积操作）与激活函数（或类似非线性操作，如 ReLU、softmax）的组合，线性变换提供基础计算，非线性操作增强模型表达能力。

不同的拟合模型的区别在于：函数的组成结构，以及线性函数与激活函数的参数不同：

- **函数结构**决定了模型的基本计算模式和特征处理能力，通过线性变换的数量、激活函数类型及连接方式体现。
  * 线性函数的变量数（神经元数量或维度）：决定模型容量和复杂度。
  * 非线性函数（**激活函数**）类型：决定模型表达能力，如 ReLU、sigmoid。
  * 连接方式：决定信息流动和特征提取模式，如全连接（MLP）、局部连接（CNN）、注意力机制（Transformer）
  * **函数结构**是由开发者根据不同的任务需要预先设计
- **学习参数**指在训练过程中可以根据拟合过程进行动态调整的参数，也就是函数中除了未知变量的可调系数。
  * 可调系数（权重、偏置）：通过训练优化，决定模型对数据的具体拟合。
  * 数量和结构随模型类型不同，如 CNN 参数较少（权重共享），MLP 参数较多。
  * 学习参数是机器学习或训练的核心，整个训练的目的就是要学习到一组合适的**学习参数**(模型函数的系数)
- **预置参数（超参数)**：由开发人员设置的参数，在训练及推理过程中不能改变的结构化参数，神经网络的层数等。
  * 结构化设计参数：在训练前设置，如层数、神经元数量、滤波器大小、注意力头数。
  * 决定模型的整体架构和行为，无法通过训练自动调整。
  * 超参数也是由不同的任务及数据预先设计，但在训练中可以由开发者手动调整
- **训练参数**决定了优化过程的效率和效果，直接影响模型收敛性和性能。
  * 优化过程相关参数：如**损失函数**定义、学习率及学习方式、迭代次数、批量大小。
  * 决定训练效率和模型收敛性，随任务和数据特性调整。
  * 超参数也是由不同的任务及数据以及训练环境而预先设计，训练过程中可以由开发者手动调整

以三种典型神经网络模型（MLP、CNN、Transformer）为例，展示上述区别的实际体现：

| 模型                  | 函数结构                                                                             | 学习参数                                     | 超参数示例                             | 训练参数示例                                     |
| --------------------- | ------------------------------------------------------------------------------------ | -------------------------------------------- | -------------------------------------- | ------------------------------------------------ |
| **MLP**         | 全连接层，ReLU 激活，<br />层级堆叠（如 3 层：784-256-10）                           | 权重矩阵W**W**、<br />偏置b**b** | 隐藏层数（3）、<br />神经元数（256）   | 交叉熵损失、<br />学习率 0.001、<br />100 epoch  |
| **CNN**         | 卷积层（局部连接），<br />ReLU 激活，池化层，<br />卷积-池化交替（如 5 层卷积+池化） | 卷积核权重、<br />偏置                       | 滤波器大小（3x3）、<br />通道数（64）  | MSE 损失、<br />Adam 优化、<br />batch size 32   |
| **Transformer** | 注意力机制（QKV+softmax），<br />MLP 前馈网络，多层编码器/解码器                     | 注意力权重、<br />前馈网络权重               | 注意力头数（8）、<br />编码器层数（6） | 交叉熵损失、<br />warm-up 学习率、<br />50 epoch |

##### 训练模型

我们知道深度神经网络由$L$层神经网络（线性函数+激活函数）构层，第$k$层的数学方程如下：

$$
y^{(k)}=\phi^{(k)}(W^{(k)}y^{(k-1)}+b^{(k)})
$$

$$
y^1=W^1 X+b^1
$$

$$
Y=y^{L}
$$

整合后即为如下方程：

$$
Y = \phi_L(W_L\phi_{L-1}(W_{L-1} \cdots \phi_1(W_1X + b_1) \cdots + b_{L-1}) + b_L)
$$

训练的过程求所有**学习参数** $W^k, b$$^k$，$k=(1..L)$的过程，设这些参数的个数是$m$个，那么理论上将至少m个$(X,Y)$代入，建立$m$个方程组，从而解出$m$个未知的学习参数。

但实际的情况不允许我们这样做：

- 方程太复杂，没有办法求出解析解
- 数据量很大，我们的目的不是求解一组学习参数，而且找到良好拟合整个训练数据集的学习参数

因此，针对深度神经网络的复杂拟合函数，一般基于控制论的最优化方法用数值方法来求解未知的学习参数，如下图：

![1746090976472](image/1.机器学习与神经网络/1746090976472.png)

其中参数定义及迭代过程如下：

- $W_0$为初始化参数
- $Y_i$为第$i$次迭代时由$W_i$参数计算出的预测值
- $L_i$为根据定义的评估函数Loss计算出的预测值与实际值的差异
- 反馈迭代时需要根据Loss最小化的方向调整参数$W_i$得到新的$W_{i+1}$，进行下一次迭代计算
- 迭代直到找到Loss的最小值（或者基于现实情景的较小值）

基于以上模型，从数学理论上讲核心目标就是**求用损失函数的极小值**。

损失函数是一个关于参数$W$及$(X,Y)$的复合函数，我们需求解此复合函数的极小值：

$$
\mathrm{arg}\min_{W} L(F(W,X), Y)
$$

在训练时，$(X,Y)$为已知参数，所以我们只需求损失函数针用参数$W$的变化的最小值，数学中导数代数针对某一参数的变化率，当此参数的变化率为0时，代表此处有一极值。

因为$W$是由多个参数组成的参数向量$(w_1,w_2,\cdots,w_n)$，所以需要对每个参数求导数（此时其它变量看作常数，此导数即偏导），并将这些变量的偏导形成对应的导数向量，此导数向量称向**梯度向量**，**梯度向量的方向代表此点的$W$参数综合变化率最大的方向**。

$$
\nabla{L_{W}}=\frac{\partial L}{\partial W}=\left( \frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2}, \ldots, \frac{\partial L}{\partial w_n} \right)
$$

##### 梯度下降

梯度$\nabla{L_W}$代表在某一参数位置点$W$综合变化率最大的方向，将损失函数可视化成三维图后可以形象化地理解为“**坡度最陡**的方向"，所以要求损失函数$\mathrm{arg}\min_{W} L(F(W,X), Y)$ 的最小值，我们需要向梯度相向的方向移动，直至找最小值时的参数$W_m$，这就是**梯度下降法**。

![1746093495972](image/1.机器学习与神经网络/1746093495972.png)

![1746094294427](image/1.机器学习与神经网络/1746094294427.png)

##### 反向传播与链式法则

在进行训练时，分为前向传播及反向传播两个迭代循环过程：

- (1) 前向传播：根据输出计算误差：
  - 随机设定初始化参数$w_{11},w_{12},w_{1n},w_{21},w_{22},w_{2n}$
  - 根据输入$X=(x_1,x_2,x_n)$向前计算出误差值$e$
- (2) [反向传播](https://www.cnblogs.com/charlotte77/p/5629865.html)：
  - 根据误差计算变化率（全局梯度$\nabla$)
  - 通过导数的链式法则，将全局梯度$\nabla$分配到每个函数结点，这是此函数结点的局部变化率（局部梯度$\delta$）
  - 根据函数的局部梯度$\delta$，调节此函数的每个参数
- 持续(1)(2)迭代到预先设定的稳定条件为止

![1746149569715](image/1.机器学习与神经网络/1746149569715.png)

反向传播的最核心的原理是求导数的链式法则，以上图的求$w_{11}$为例，$w_{11}$的变化率为：

因为损失函数$L$相对于参数$w_{11}$的方程为：

$$
L=\phi_2(S_2(\phi_1(S_1(w_{11}))))
$$

根据复合函数求导的链式法则，对于神经元$S1$的局部梯度$\delta_{S_1}$为：

$$
\delta_{S_1}=\frac {\partial L}{\partial S_{1}}=\frac {\partial L}{\partial \phi_{2}} \frac {\partial \phi_2}{\partial S_{2}}\frac {\partial S_2}{\partial \phi_{1}}\frac {\partial \phi_1}{\partial S_{1}}
$$

根据复合函数求导的链式法则，$w_{11}$的变化率为：

$$
\frac {\partial L}{\partial w_{11}}=\frac {\partial L}{\partial \phi_{2}} \frac {\partial \phi_2}{\partial S_{2}}\frac {\partial S_2}{\partial \phi_{1}}\frac {\partial \phi_1}{\partial S_{1}}\frac {\partial S_1}{\partial w_{11}}=\delta_{S_1}x_1
$$

针对全连连神经网络，由链式法则推导出的局部梯度 $\delta$公式为

$$
\delta_{j}^{(l)}(n) = 

\begin{cases}

\phi_{j}'(S_{j}^{(L)}(n))\ e_{j}^{(L)}(n) & \text{对输出层 } L \text{ 的神经元 } j \\

\phi_{j}'(S_{j}^{(l)}(n))\ \sum_{k}\delta_{k}^{(l+1)}(n)w_{kj}^{(l+1)}(n) & \text{对隐藏层 } l \text{ 的神经元 } j

\end{cases}
$$

这里 $\phi_{j}'(\cdot)$ 是指对自变量的导数，对于常见的激活函数，导数可以简化线性运算，不用作复杂的求导操作，比如Logistic激活函数$\phi(y)=\frac{1}{1+e^{-ax}}$，其导数线性化为$\phi'=ay_i(n)(1-y_j(n))$

根据广义 delta 规则调节网络第 $l$ 层的突触权值：

$$
w_{ji}^{(l)}(n + 1) = w_{ji}^{(l)}(n) - \eta\delta_{j}^{(l)}(n)y_{i}^{(l-1)}(n)
$$

这里 $\eta$ 为学习率参数(单位调节步长)，$\alpha$ 为动量常数。

这里局部梯度$\delta$在整个神经网络中从后向前传播，每一层的$\delta^{l}$由后一层的$\delta^{l+1}$算出，即由$l+1$层传向$l$层，这就是**反向传播算法**。然后每层的更新参数$w_{ji}$可以由本层的输入(针对隐藏层就是上一层的输出$y^{l-1}$)本层局部梯度$\delta$计算得出。

##### 损失函数

损失函数（Loss Function），也称为代价函数（Cost Function）或目标函数（Objective Function），是机器学习和深度学习中用于衡量模型预测结果与真实标签之间差异的核心工具。损失函数量化了模型输出的错误程度，通过最小化损失函数，模型可以调整学习参数（如权重和偏置）以提高预测准确性。以下是对损失函数的全面介绍，涵盖其定义、作用、常见类型、设计原则以及在深度学习中的应用。

- **均方误差（Mean Squared Error, MSE）**：

  - 定义：预测值与真实值差的平方的平均值：

    $$
    L_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2
    $$

    其中 $n$ 是样本数量，$\hat{y}_i$ 是预测值，$y_i$ 是真实值。
  - 特点：对大误差的惩罚更严厉（平方项放大误差），适用于预测值需要与真实值尽可能接近的场景，如房价预测。
  - 优点：数学上简单，易于求导，便于梯度下降优化。
  - 缺点：对异常值（outliers）敏感，异常值会导致损失值异常增大。
- **平均绝对误差（Mean Absolute Error, MAE）**：

  - 定义：预测值与真实值差的绝对值的平均值：
    $$
    L_{\text{MAE}} = \frac{1}{n} \sum_{i=1}^{n} |\hat{y}_i - y_i|
    $$
  - 特点：对误差的惩罚是线性的，比 MSE 对异常值更为鲁棒。
  - 优点：对异常值不敏感，适用于数据中存在噪声或异常值的场景。
  - 缺点：不可导（在零点处不可微），可能影响优化稳定性。
- **交叉熵损失（Cross-Entropy Loss）**：

  - 定义：衡量模型预测概率分布与真实标签分布之间的差异，常用于多分类问题。对于单个样本：

    $$
    L_{\text{CE}} = - \sum_{c=1}^{C} y_c \log(\hat{y}_c)
    $$

    其中 $C$ 是类别数，$y_c$ 是真实标签的 one-hot 编码（如果是类别 $c$，则为 1，否则为 0），$\hat{y}_c$ 是模型预测的概率（通常通过 softmax 得到）。
  - 对于二分类问题，可简化为：

    $$
    L_{\text{BCE}} = - [y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
    $$

    称为二元交叉熵（Binary Cross-Entropy, BCE）损失。
  - 特点：鼓励模型预测正确的类别概率接近 1，错误类别概率接近 0。
  - 优点：与 softmax 输出结合，适用于多分类任务；梯度计算简单。
  - 应用：图像分类、文本分类等。
- **正则化**

  - **L2 正则化（Ridge Regularization，权重衰减）**
- - - **定义**：通过对模型参数的 L2 范数（即参数平方和）施加惩罚，添加到损失函数中：

      $$
      R(\theta) = \|\theta\|_2^2 = \sum_{i} \theta_i^2
      $$

      完整的损失函数为：

      $$
      L_{\text{regularized}} = L_{\text{original}} + \lambda \|\theta\|_2^2
      $$
    - **特点**：鼓励参数取较小的值，但不会强制参数变为 0，使得参数分布更平滑。
    - **效果**：减少模型对某些特征的过度依赖，防止权重过大导致过拟合。
    - **应用**：广泛用于线性回归（Ridge 回归）、神经网络等。
  - **L1 正则化（Lasso Regularization）**

    - **定义**：通过对模型参数的 L1 范数（即参数绝对值之和）施加惩罚：

      $$
      R(\theta) = \|\theta\|_1 = \sum_{i} |\theta_i|
      $$

      完整的损失函数为：

      $$
      L_{\text{regularized}} = L_{\text{original}} + \lambda \|\theta\|_1
      $$
    - **特点**：倾向于使部分参数变为 0，具有特征选择（Feature Selection）的效果。
    - **效果**：通过将不重要特征的权重置为 0，简化模型，增强模型可解释性。
    - **应用**：常用于线性回归（Lasso 回归）、稀疏模型构建。
  - **弹性网络正则化（Elastic Net Regularization）**

    - **定义**：结合 L1 和 L2 正则化的优点，同时施加 L1 和 L2 范数的惩罚：

      $$
      R(\theta) = \alpha \|\theta\|_1 + (1-\alpha) \|\theta\|_2^2
      $$

      完整的损失函数为：

      $$
      L_{\text{regularized}} = L_{\text{original}} + \lambda \left( \alpha \|\theta\|_1 + (1-\alpha) \|\theta\|_2^2 \right)
      $$
    - **特点**：既能产生稀疏解（通过 L1），又能平滑参数（通过 L2）。
    - **效果**：平衡特征选择和平滑性。
    - **应用**：适用于特征相关性较高的情况，如高维数据分析。

##### 激活函数

激活函数（Activation Function）是神经网络中的核心组成部分，用于在每一层神经元中引入非线性变换，从而增强模型对复杂数据的建模能力。激活函数引入的非线性变换是神经网络能拟合任何函数的关键。

- **Sigmoid 函数**

  - **定义**：将输入映射到 (0, 1) 区间内：
    $$
    f(z) = \frac{1}{1 + e^{-z}}
    $$
  - **特点**：输出范围为 (0, 1)，可以解释为概率；当 $z$ 很大或很小时，函数趋于饱和（接近 1 或 0）。
  - **优点**：输出可解释性强，适合二分类任务的输出层。
  - **缺点**：存在梯度消失问题（当 $z$ 远离 0 时，梯度接近 0，导致反向传播时梯度无法有效传递）；输出不以 0 为中心，可能影响优化。
  - **应用**：早期神经网络常用，现多用于输出层（如二分类 logits 到概率的转换）。
- **Tanh 函数（双曲正切函数）**

  - **定义**：将输入映射到 (-1, 1) 区间内：
    $$
    f(z) = \tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}
    $$
  - **特点**：输出范围为 (-1, 1)，以 0 为中心，相比 sigmoid 更对称。
  - **优点**：输出以 0 为中心，有助于优化收敛；比 sigmoid 梯度稍大。
  - **缺点**：仍然存在梯度消失问题，饱和区域梯度接近 0。
  - **应用**：常用于隐藏层，早期 RNN（如 LSTM 的变体）中使用较多。
- **ReLU 函数（Rectified Linear Unit，修正线性单元）**

  - **定义**：将负输入置为 0，正输入保持不变：
    $$
    f(z) = \max(0, z)
    $$
  - **特点**：非线性函数，非常简单，计算效率高；输出范围为 [0, ∞)。
  - **优点**：缓解梯度消失问题（正输入区域梯度为 1）；稀疏激活（负输入被置为 0，部分神经元不活跃），有助于减少过拟合；计算速度快。
  - **缺点**：存在“神经元死亡”问题（即负输入神经元永远不更新）；输出不以 0 为中心。
  - **应用**：目前深度学习中最常用的激活函数，广泛用于卷积神经网络（CNN）和全连接网络的隐藏层。
- **Softmax 函数**

  - **定义**：用于多分类任务，将一组值转换为概率分布：

    $$
    f(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}, \quad i=1,2,...,C
    $$

    其中 $C$ 是类别数，输出总和为 1。
  - **特点**：输出范围为 (0, 1)，且所有类别输出之和为 1，可解释为概率分布。
  - **优点**：适合多分类任务，常与交叉熵损失结合使用。
  - **缺点**：不适用于隐藏层，计算成本较高。
  - **应用**：专用于输出层，用于多分类概率输出。

##### 初始学习参数

采用数值方法——即迭代的方法（或系统反馈的方法）来求解模型函数，其中的初始参数（或叫初始状态）很关键，设置合适的初始化参数会加速迭代过程，使其快速收敛。而不当的初始参数可能会让迭代收敛速度变慢，甚至不能收敛。

神经网络的**初始化参数**对其训练过程和最终性能有重要影响。合理的初始化可以加速收敛、避免梯度消失或梯度爆炸问题，而不合适的初始化可能导致训练失败或性能下降。

神经网络是一个复杂的函数，由多层线性函数及激活函数组成，有不同的初始化方案针对不同的神经网络架构：

| **初始化方法**    | **适用激活函数** | **优点**                   | **缺点**                       |
| ----------------------- | ---------------------- | -------------------------------- | ------------------------------------ |
| **零初始化**      | 偏置（bias）           | 简单易用                         | 权重初始化为零会导致对称性问题       |
| **随机初始化**    | 所有激活函数           | 打破对称性                       | 需要手动调整范围，可能导致梯度不稳定 |
| **Xavier 初始化** | Sigmoid、Tanh          | 适合浅层网络，输出方差一致       | 对 ReLU 激活函数效果不佳             |
| **He 初始化**     | ReLU、Leaky ReLU       | 适合深层网络，缓解梯度消失问题   | 对 Sigmoid 和 Tanh 激活函数效果不佳  |
| **正交初始化**    | RNN、LSTM              | 保持信号幅度，适合动态变化的输入 | 计算复杂度较高                       |
| **稀疏初始化**    | 所有激活函数           | 减少参数相关性，降低过拟合风险   | 可能导致训练速度较慢                 |

### 数据处理

##### 数据的表示

神经网络的本质是用连续的函数曲线拟合数据模式，所以任何问题的输入都必需数值化。

- 数值化输入：如果不是数值化的如文本、语音、分类等信息，需要使用类似ont-hot编码等方式将其数值化。
- **一个**数据往往包含多个属性，这些属性整体组成一个有完整意义的数据，因为可以用**向量**来表示这个数据，相当于数据表中的一行中的多个属性的值表示一个完整的数据。
- 多个数据（即多行数据）可以用多个向量组合表示，即用**矩阵**来表示数据集
- 表示的数据集的矩阵叫**数据矩阵**或者叫**容器矩阵**

##### 数据的运算

在神经网络中，核心的线性函数定义为$Y=WX$，这三个参数均为矩阵，由线性代数知识，这里的$W$是线性变换矩阵（或映射矩阵，不是数据矩阵），$X$是数据矩阵，$WX$的作用是通过映射矩阵$W$将$X$映射为$Y$。

- 线性函数中的运算为线性变换，参数矩阵为**映射矩阵**，映射矩阵需要矩阵运算规则（特别是矩阵乘法规则）进行计算
- 针对映射矩阵，严格要求维度对齐，因此在设置参数时需要注意维度协调问题。
- 针对**数据矩阵**的并行批量计算，是按每个元素作同样操作的运算，由常用的软件包如numpy/pytorch提供
- 在编写程序时，需要特别注意矩阵运行是针对**容器矩阵**还是针对**映射矩阵**的计算

在神经网络中，数据可以用**容器矩阵**表示，而函数计算可以用**映射矩阵**表示，因些神经网络可以用线性代数中的矩阵进行计算，并且可以用矩阵实现高性能的并行化处理。

##### 数据的属性

###### 向量关系属性

- 点积：两个向量之间的运算，结果是一个标量。点积衡量了两个向量在同一方向上的“投影”程度，反映了它们之间的相似性或相关性。

$$
\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \dots + a_n b_n=\mathbf{a}^T  \mathbf{b}=\|\mathbf{a}\| \cdot \|\mathbf{b}\| \cdot \cos(\theta)
$$

    点积可以用来判断两个向量的方向关系（同向、垂直/正交、反向）

- 范数：范数是用来衡量向量“大小”或“长度”的函数，表示向量在空间中的尺度，通常用于描述距离或强度。

$$
\|\mathbf{x}\| = \sqrt{\sum_{i=1}^{n} x_i^2}=\sqrt{x \cdot x}=\sqrt{x^Tx}
$$

###### 中心位置测量

中心位置指标用于描述数据的“典型值”或“中心点”，反映数据分布的集中位置。

- **均值（Mean/Average）：**数据的平均值，表示所有数据点的算术平均，对异常值敏感，适用于近似正态分布的数据

  $$
  \mu = \frac{1}{n} \sum_{i=1}^{n} x_i
  $$

  其中 \( x_i \) 是第 \( i \) 个数据点，\( n \) 是数据点总数。
- **中位数（Median）**：将数据从小到大排序，若数据点个数为奇数，中位数是中间值；若为偶数，则取中间两个值的平均值。

###### 离散程度测量

离散程度指标用于描述数据的分布范围及其离中心位置的分散程度。

- **方差（Variance）**：衡量数据点偏离均值的平均平方偏差，反映数据的波动大小，值越大，数据越分散

  $$
  \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2
  $$

  （样本方差通常用 \( \frac{1}{n-1} \) 以获得无偏估计）。
- **标准差（Standard Deviation）**：方差的平方根，与原始数据单位相同，更直观地表示数据分散程度，适用于正态分布数据的分析

  $$
  \sigma = \sqrt{\sigma^2} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2}
  $$
- **四分位距（Interquartile Range, IQR）**：四分位距是中间 50% 数据的范围，对异常值不敏感，反映数据集中部分的分布范围

  $$
  \text{IQR} = Q3 - Q1
  $$

  其中 \( Q1 \) 是第 25% 分位数，\( Q3 \) 是第 75% 分位数。

###### 分布形状测量

分布形状指标用于描述数据分布的对称性和尖锐程度。

- **偏态（Skewness）：**偏态衡量数据分布的对称性，反映数据是否偏向均值的一侧。偏态 > 0 为右偏（正偏态），偏态 < 0 为左偏（负偏态），偏态 ≈ 0 为近似对称

  $$
  \text{Skewness} = \frac{\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^3}{\left( \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2 \right)^{3/2}}
  $$
- **峰态（Kurtosis）**：峰态衡量数据分布的尖锐程度，反映分布的“尾部厚度和峰值高低”。峰态 > 0 为高尖分布，峰态 < 0 为低平分布，峰态 ≈ 0 接近正态分布

  $$
  \text{Kurtosis} = \frac{\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^4}{\left( \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2 \right)^2} - 3
  $$

###### 其他统计属性

- **分位数（Quantiles）**

  - **公式/计算方法**：将数据分为若干等份的值，如四分位数（\( Q1, Q2, Q3 \)）将数据分为四份，百分位数将数据分为百份。
  - **简介**：分位数用于描述数据分布位置，反映特定百分比的数据点，例如中位数是 50% 分位数。
- **协方差（Covariance）**：协方差衡量两个变量间的线性相关性，正值表示正相关，负值表示负相关，值的大小反映相关强度

  $$
  \text{Cov}(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu_x)(y_i - \mu_y)
  $$
- **皮尔逊相关系数（Correlation Coefficient）**：是标准化的协方差，范围在 [-1, 1]，用于衡量两个变量的线性相关强度和方向，1 表示完全正相关，-1 表示完全负相关，0 表示无线性相关。

  $$
  r = \frac{\text{Cov}(X, Y)}{\sigma_x \sigma_y}
  $$

##### 数据的处理

###### 标准化与归一化

神经网络模型的数据用向量表示，每个向量的分量代表数据的一个特性或一个数据维度。但是向量分量的数据值可能差异很大，这会导致在作拟合时计算上的困难，比如出现计算溢出、梯度爆炸或梯度消失等现象。

实际上向量分量的数据值可以看成基于某一量纲的测量，为了将数据值限制在一定范围，我们可以选择或自定义量纲，让测量值在某一范围之内，这样并不会对数据的分布与意义启成影响，但会便于使用计算机进行运算。

一种常用的方法是数据的标准化（Standardization)，即将数据转化为均值为0，标准差为1的分布。

另一种的方法是将数据压缩到[0,1]区间，即实现数据的归一化（Normalization)。

| 特性                   | 标准化（Standardization）                       | 归一化（Normalization）                    |
| ---------------------- | ----------------------------------------------- | ------------------------------------------ |
| **目标**         | 将数据转换为均值为 0、标准差为 1 的分布         | 将数据缩放到固定范围（如 [0, 1]）          |
| **数学方法**     | $x'=\frac{x-\mu}{\sigma}$                     | $x'=\frac{x-x_{min}}{x_{max}-x_{min}}$   |
| **输出范围**     | 无固定范围，通常符合正态分布                    | 固定范围，通常为 [0, 1] 或 [-1, 1]         |
| **分布影响**     | 保留原始分布，仅调整均值和标准差                | 改变相对分布，取决于数据的最小值和最大值   |
| **对异常值影响** | 相对鲁棒，受异常值影响较小                      | 敏感，异常值可能压缩大部分数据范围         |
| **适用场景**     | 适合假设正态分布或距离相关的模型（如 SVM、KNN） | 适合范围敏感的模型（如神经网络、图像处理） |
| **计算复杂度**   | 需要计算均值和标准差                            | 仅需计算最大值和最小值                     |

###### 降维

降维是将高维数据（即具有较多特征或维度的数据）映射到低维空间的过程，同时尽可能保留原始数据的重要信息或结构。降维的目标是减少数据的维度（特征数量），从而简化数据表示。

降维的两种主要方式

* **特征选择（Feature Selection）** ：从原始特征中选择一个子集，丢弃不重要的特征，属于降维的一种形式，但不涉及特征变换。
* **特征提取（Feature Extraction）** ：通过数学变换将原始高维特征组合成新的低维特征，通常涉及投影方法（如 PCA、t-SNE）。

主成分分析PCA是常见的降维方法，通过找到数据方差最大的方向（主成分）进行投影，将数据映射到低维空间，基本原理如下：

* 计算数据的协方差矩阵$\sum=\frac{1}{n}X^TX$
* 对协方差矩阵进行特征值分解，得到特征值和特征向量
* 按特征值大小排序，选择前 k**k** 个特征向量组成投影矩阵$W$
* 投影：$Y=XW$

##### 数据集拆分

在神经网络的训练和评估过程中，数据集拆分（Dataset Splitting）是一个关键步骤，用于确保模型能够有效学习、验证和测试其性能。合理拆分数据集可以帮助评估模型的泛化能力、防止过拟合，并为模型调优提供依据。

* **训练集（Training Set）** ：用于模型训练，优化权重参数。模型会反复迭代学习这部分数据。
* **验证集（Validation Set）** ：用于模型选择、超参数调优和早期停止（Early Stopping）。验证集帮助监控训练过程中的过拟合情况。
* **测试集（Test Set）** ：用于最终评估模型性能，仅在训练和调优完成后使用，确保评估结果反映模型对未知数据的泛化能力。

拆分比例没有固定标准，通常根据数据集大小、任务复杂度和计算资源选择。以下是一些常见比例和适用场景：

| 拆分方式             | 训练集 | 验证集 | 测试集 | 适用场景                     |
| -------------------- | ------ | ------ | ------ | ---------------------------- |
| 基本拆分（大数据集） | 70%    | 15%    | 15%    | 数据量充足，深度学习常规任务 |
| 基本拆分（小数据集） | 80%    | 10%    | 10%    | 数据量较少，需更多训练数据   |
| 交叉验证（k=5）      | 80%    | 20%    | 无     | 数据量少，传统机器学习任务   |
| 训练-测试拆分        | 80%    | 无     | 20%    | 简单任务或无需调优           |
| 时序数据拆分         | 60%    | 20%    | 20%    | 时间序列预测任务             |

### 评估与调节

##### 过拟合与欠拟合

![1746185688361](image/1.机器学习与神经网络/1746185688361.png)

如同前面对学习的解析，学习分为二个部分：由数据建模形成知识模型的部分，即神经网络的训练部分。二是将生成的数据模型应用于未知数据进行预测或推理的部分。

最终的目标是两个过程的综合，即在未知数据的预测中有良好的表现，而不只是训练模型在已知的数据上表现良好，这涉及到训练模型的自由度及泛化问题。

模型训练过程中，拟合方案需要同时考虑到建模部分（即适配训练数据的学习部分），也需要考虑模型应用部分（即适配未知数据的预测部分），二者平衡才是适当的拟合。

有二种偏颇的拟合会导致最终效果不好，这是在训练过程中需要避免的现象：

- 过拟合：过拟合是指过度拟合训练数据，也就是模型函数很精确地拟合了训练数据，模型过于复杂，而缺乏自由度与灵活性，导致在测试数据或未知数据中表现不好
- 欠拟合：欠拟合是指模型没有良好地拟合训练数据，即拟合不足，模型太过简单，拥有太多的自由度，也会导致模型在测试数据集的预测中表现不好

综合考虑机器学习的训练及应用过程，在学习的目的是为了应用，学习的过程是面对确定的场景（已定义好的数据），而应用的过程是面对未知的场景（没有定义或标注的数据），最终的评价是我们需要在未知场景中有良好的预测能力，这种能力就是泛化能力，这种能力是约束与自由度的良好平衡。

约束要求从训练数据中尽可能提炼出良好的模型结构，即尽可以良好的拟合曲线，要达成这些约束，要求：

- 尽可以有代表性的且一定数量的训练数据，只有足够的数据且相关的数据才能提炼出模型结构
- 选择拟合能力强的模型：神经网络的层次越深、每层神经元的数量越多，拟合能力就越强。激活函数与损失函数也需要针对不同任务作选择
- 迭代过程中的收敛程度：迭代次数越多，收敛变化越小，代表拟合程度越好
- 使用残差连接

约束过强会减少模型的自由度，从而影响模型在未知数据集上的预测表现，所以需要在训练过程中保留一定的自由度：

- 训练数据尽可能多样化、随机化、数据分布丰富，而不是单一的训练数据。在训练时可以使用交叉验证等方法
- 神经网络的深度及神经元的数量不能太多，否则容易出现过拟合，甚至可以在训练过程中使用dropout技术人为丢弃一部分神经元，也可以使用正则化方法对参数进行限制约束
- 迭代过程收敛不定太好，这样容易出现过拟合，可以在迭代指定次数后即停止
- 某些情况下主动丢弃一些神经元（Dropout方法）
- 损失函数加上正则化约束

所以与传统的编程的设计后应用、保证约束与精确的过程不一样，机器学习的训练与应用过程是一个系统反馈调节的过程，也是约束与自由、精确与模糊的平衡的过程，这是中国"中庸之道"的修行与应用。

### 训练步骤

##### 数据准备

- **数据预处理**：包括标准化（均值为 0，标准差为 1）、归一化（范围缩放到 [0, 1]）、数据增强等，确保数据适合神经网络输入。
- **数据集拆分**：将数据集划分为训练集（用于参数学习）、验证集（用于超参数调优）和测试集（用于最终评估），常见比例为 70%:15%:15%。

##### 模型初始化

- **网络结构**：定义神经网络的层数、每层神经元数量、激活函数（如 ReLU、Sigmoid）等。
- **参数初始化**：权重 $W$通常采用随机初始化（如均匀分布、正态分布）或预训练模型。常见方法包括 Xavier 初始化和 He 初始化，避免梯度消失或爆炸。

##### 前向传播

- **输入到输出**：数据通过网络层逐层计算，得到预测输出。

##### 损失计算

- **损失函数**：衡量模型预测值与真实值之间的误差，指导优化方向。常见损失函数包括：

  - 回归任务：均方误差（MSE）：

    $$
    L = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2
    $$
  - 分类任务：交叉熵损失（Cross-Entropy Loss）：

    $$
    L = -\frac{1}{m} \sum_{i=1}^{m} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
    $$
- **正则化项**：为防止过拟合，常在损失函数中加入正则化项（如 L1、L2 范数）：

  $$
  L_{\text{total}} = L + \lambda \cdot \text{Regularization Term}
  $$

##### 反向传播

- **目标**：基于损失函数对模型参数计算梯度，确定参数调整方向。
- **链式法则**：通过链式法则，从输出层到输入层逐层计算梯度：

  $$
  \frac{\partial L}{\partial \mathbf{W}^{[l]}} = \frac{\partial L}{\partial \mathbf{z}^{[l]}} \cdot \frac{\partial \mathbf{z}^{[l]}}{\partial \mathbf{W}^{[l]}} = \frac{\partial L}{\partial \mathbf{z}^{[l]}} \cdot \mathbf{a}^{[l-1]}
  $$
- **计算复杂度**：反向传播使梯度计算高效，是神经网络训练的核心。

##### 参数更新

- **梯度下降（Gradient Descent）**：根据梯度调整参数，使损失函数值减小：

  $$
  \mathbf{W}^{[l]} \leftarrow \mathbf{W}^{[l]} - \eta \cdot \frac{\partial L}{\partial \mathbf{W}^{[l]}}
  $$
- **优化算法**：常用优化算法包括：

  - **随机梯度下降（SGD）**：每次使用小批量（Mini-Batch）数据更新参数，降低计算成本。
  - **动量法（Momentum）**：引入动量项加速收敛：

    $$
    \mathbf{v}_t = \beta \cdot \mathbf{v}_{t-1} + (1 - \beta) \cdot \nabla L
    $$

    $$
    \mathbf{W} \leftarrow \mathbf{W} - \eta \cdot \mathbf{v}_t
    $$

    其中 $\beta $ 是动量参数。
  - **Adam 优化器**：结合动量和自适应学习率，基于一阶动量（均值）和二阶动量（方差）更新参数，适合大多数深度学习任务。

##### 迭代优化

- **Epoch 和 Batch**：
  - 一个 Epoch 表示完整遍历一次训练集。
  - 训练集通常分为多个 Batch，每个 Batch 更新一次参数。
- **终止条件**：训练迭代直至达到以下条件之一：
  - 预设的 Epoch 数量。
  - 损失函数收敛（损失变化小于阈值）。
  - 验证集性能不再提升（Early Stopping，早期停止）。

##### 验证与测试

- **验证集**：在训练过程中，定期用验证集评估模型性能，调整超参数（如学习率、正则化参数），监控过拟合。
- **测试集**：训练结束后，用测试集评估模型最终性能，确保泛化能力。

## 三、神经网络应用模型

### 神经网络语义原理

从学习原理出发，我们将神经网络的学习分成两大部分：

- **理解过程**将输入信息转换成语义模型：$输入信息 \longrightarrow 结构化知识（语义空间/语义模型）$
- **生成过程**将语义模型转换为输出信息：$结构化知识（语义空间/语义模型）\longrightarrow 输出信息$

神经网络中的所有学习参发综合起来代表一种抽象的**语义模型**，我们可以通过**语义模型**作为中间层，完成输入到输出的任意转换。

对习学习而言，如何**理解**是学习的核心，对于神经网络而言，将输入信息整合成语义模型的不同方法是不同学习的根本区别。

因此基于语义模型的处理方式将神经网络分为以下几类：

* **局部汇总语义模型 (Local Expansion Semantic Models)**

  * **定义** ：通过从局部特征开始，逐步扩展到更广范围的语义构建信息处理过程，主要应用于空间数据。
  * **典型模型** ：卷积神经网络 (CNN)，如 ResNet、VGG。
  * **特点** ：通过卷积和池化操作，从局部到整体构建语义特征。
  * **应用场景** ：图像分类、目标检测、图像分割。
  * **阅读类比**：第一遍读书记整理局部笔记1，第二编读笔记1整理笔记2，直到理解整书全意。
* **顺序加入语义模型 (Sequential Addition Semantic Models)**

  * **定义** ：通过按时间步或序列顺序逐步加入新信息，累积上下文语义，强调顺序依赖性。
  * **典型模型** ：循环神经网络 (RNN)、长短期记忆网络 (LSTM)、门控循环单元 (GRU)。
  * **特点** ：利用循环机制，基于历史信息更新语义状态。
  * **应用场景** ：自然语言处理、时间序列预测、语音识别。
  * **阅读类比**：顺序阅读第一章并完全理解，再读第二章完全理解，直到读完理解整书全意。
* **全局关联语义模型 (Global Correlation Semantic Models)**

  * **定义** ：通过计算输入各部分之间的全局关联，直接构建整体语义特征，强调全局交叉性。
  * **典型模型** ：Transformer (如 BERT、GPT、Vision Transformer)。
  * **特点** ：使用自注意力机制，一次性捕捉全局语义。
  * **应用场景** ：机器翻译、文本生成、图像处理。
  * **阅读类比**：多人乱序阅读并交叉分析各章关连，整合所有人的阅读后，每人理解整书全意。

### 局部扩展语义模型

##### [卷积神经网络CNN](http://hejunhao.me/archives/1364)

**连续域中的卷积**：

- 卷积用于描述两个函数的“交互”或“过滤”关系。对于两个连续函数 $ f(t) $ 和 $ g(t) $，卷积定义为：

  $$
  (f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau)d\tau
  $$

  其中：

  - $ f(\tau) $ 是输入信号。
  - $ g(t - \tau) $ 是翻转后的核函数（或滤波器）。
  - 输出 $ (f * g)(t) $ 是输入信号经过核函数滤波后的结果。
- 直观理解：核函数 $ g $ 在输入信号 $ f $ 上滑动，计算每一位置的重叠加权和，用于提取特定模式或特征。

**卷积的几何与直观解释**

- **滑动窗口** ：卷积操作可以看作一个滑动窗口过程：卷积核（滤波器）在输入数据（图像或信号）上逐位置滑动，计算每个位置的局部邻域与核的加权和。例如，在图像处理中，一个 3x3 的边缘检测核（如索贝尔核）滑动到每个像素，计算局部梯度值，用于提取边缘特征。
- **特征提取** ：卷积核的权重定义了某种特征模式（如水平边缘、垂直边缘）。通过卷积操作，输入中匹配该模式的区域会产生较高的输出值，从而实现了特征提取。
- **翻转核的作用** ：数学上卷积需要翻转核 $g(t−τ)$，这是为了保持操作的对称性和数学一致性，但在神经网络（如 CNN）中常省略翻转，直接用核权重计算（称为“互相关”，但仍称作卷积）。

**神经网络中的卷积**

在卷积神经网络（CNN）中，卷积操作是核心组件，用于处理空间数据（如图像）。其数学意义和应用如下：

- **局部感受野：**
  - 卷积操作只关注输入的局部邻域（如 3x3 区域），降低了参数量（相比全连接层），体现了“局部连接”思想。
  - 数学上，一个卷积层的输出特征图为：

    $$
    O[i, j] = \sum_{p=-k}^{k} \sum_{q=-k}^{k} I[i + p, j + q] \cdot W[p, q] + b
    $$

    其中：

    - $ I $ 是输入图像，$ O $ 是输出特征图。
    - $ W $ 是卷积核（大小为 $(2k + 1) \times (2k + 1)$）。
    - $ b $ 是偏置项。
- **权重共享** ：同一卷积核在整个输入上共享权重，进一步减少参数量，提高计算效率，同时使得模型对位置无关的特征有鲁棒性。
- **多核与特征提取** ：CNN 使用多个卷积核，每个核学习不同的特征（如边缘、纹理），数学上每个核生成一个特征图，组合后形成多通道输出。
- **池化与降维** ：卷积后通常跟随池化操作（如最大池化），从数学上看是局部区域的降维操作，进一步提取主导特征，减少计算量。

卷积神经网络是利用数学上的卷积特性，利用局部性原理，逐层提取输入序列的特征属性，而获取输入序列的全体的语义空间的神经网络。

![1746233295929](image/1.机器学习与神经网络/1746233295929.png)

**卷积核(或称滤波器，filter/kernel)**

![1746233488433](image/1.机器学习与神经网络/1746233488433.png)

**多通道卷积**

![1746233619892](image/1.机器学习与神经网络/1746233619892.png)

当输入有多个通道（channel）时(例如图片可以有 RGB 三个通道)，卷积核需要拥有相同的channel数,每个卷积核 channel 与输入层的对应 channel 进行卷积，将每个 channel 的卷积结果按位相加得到最终的 Feature Map。

**池化层**

![1746233685932](image/1.机器学习与神经网络/1746233685932.png)

池化层主要对卷积层学习到的特征图进行亚采样（subsampling）处理，意义在于：
1.降低了后续网络层的输入维度，从而减少计算量。
2.增强了 Feature Map 的健壮性（Robust），防止过拟合。

**简单的数字识别卷积神经网络代码：**

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# 设置随机种子以保证结果可重复
torch.manual_seed(42)
if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

# 1. 加载和预处理 MNIST 数据集
# 定义数据预处理：将图像转换为张量并进行归一化
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # MNIST 图像均值为 0.5，标准差为 0.5
])

# 加载 MNIST 数据集 (训练集和测试集)
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# 创建数据加载器
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# 2. 定义 CNN 模型
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # 第一个卷积层：输入通道数=1 (灰度图像)，输出通道数=16，卷积核大小=3x3，步长=1，填充=1
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)
        # 最大池化层：池化核大小=2x2，步长=2
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        # 第二个卷积层：输入通道数=16，输出通道数=32，卷积核大小=3x3，步长=1，填充=1
        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)
        # 全连接层：输入维度=32*7*7 (经过两次池化后图像大小为7x7)，输出维度=120
        self.fc1 = nn.Linear(32 * 7 * 7, 120)
        # 全连接层：输入维度=120，输出维度=84
        self.fc2 = nn.Linear(120, 84)
        # 全连接层：输入维度=84，输出维度=10 (对应10个数字类别)
        self.fc3 = nn.Linear(84, 10)
        # 激活函数
        self.relu = nn.ReLU()
        # Dropout 层以防止过拟合
        self.dropout = nn.Dropout(0.25)

    def forward(self, x):
        # 输入 x 形状：[batch_size, 1, 28, 28]
        x = self.pool(self.relu(self.conv1(x)))  # 卷积+激活+池化，输出形状：[batch_size, 16, 14, 14]
        x = self.pool(self.relu(self.conv2(x)))  # 卷积+激活+池化，输出形状：[batch_size, 32, 7, 7]
        x = x.view(-1, 32 * 7 * 7)  # 展平为向量，形状：[batch_size, 32*7*7]
        x = self.relu(self.fc1(x))  # 全连接+激活
        x = self.dropout(x)  # Dropout
        x = self.relu(self.fc2(x))  # 全连接+激活
        x = self.dropout(x)  # Dropout
        x = self.fc3(x)  # 最终输出，形状：[batch_size, 10]
        return x

# 3. 实例化模型、损失函数和优化器
model = SimpleCNN().to(device)
criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数，用于分类任务
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # SGD 优化器

# 4. 训练模型
num_epochs = 5
print("开始训练...")
for epoch in range(num_epochs):
    model.train()  # 设置模型为训练模式
    running_loss = 0.0
    for i, (images, labels) in enumerate(train_loader):
        images, labels = images.to(device), labels.to(device)
  
        # 前向传播
        outputs = model(images)
        loss = criterion(outputs, labels)
  
        # 反向传播和优化
        optimizer.zero_grad()  # 梯度清零
        loss.backward()  # 计算梯度
        optimizer.step()  # 更新参数
  
        running_loss += loss.item()
        if (i + 1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')
            running_loss = 0.0

# 5. 测试模型
model.eval()  # 设置模型为评估模式
correct = 0
total = 0
with torch.no_grad():  # 禁用梯度计算
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)  # 获取预测类别
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f'测试集准确率: {accuracy:.2f}%')

```

**卷积神经网络的特点**

* **优点** ：

  * **局部特征提取** ：数学上体现了局部相关性，适合空间数据（如图像）。
  * **参数效率** ：局部连接和权重共享大幅减少参数量。
  * **平移不变性** ：对特征的位置变化有一定鲁棒性。
* **缺点** ：

  * **感受野有限** ：单层卷积感受野较小，需多层堆叠才能捕捉大范围依赖。
  * **对全局上下文不足** ：卷积关注局部，无法直接建模全局关联（需结合注意力机制等改进）

### 顺序加入语义模型

##### 循环神经网络 (RNN)

循环神经网络（RNN）的核心原理确实可以看作是一个带有内置状态的状态机（state machine）。这种状态机的设计使得 RNN 能够在处理序列数据时，通过维护和更新一个内部状态（通常称为隐藏状态，hidden state），逐步累积和传递历史信息，从而捕捉序列中的时间依赖性和上下文关系。

![1746237304109](image/1.机器学习与神经网络/1746237304109.png)

* **状态机的基本概念** ：

  * 状态机是一种计算模型，包含一组状态（states）、输入（inputs）、输出（outputs）以及状态转移规则（transition rules）。在每个时间步，状态机会根据当前状态和输入，更新到新的状态，并可能产生输出。
  * 对于 RNN，状态机体现在其内部的隐藏状态（hidden state）上，这个隐藏状态存储了历史信息，并根据当前输入和状态转移规则进行更新。
* **RNN 的内置状态** ：

  * RNN 的“内置状态”就是隐藏状态 ht**h**t，它在每个时间步 t**t** 被更新，包含了从序列开始到当前时间步的所有输入信息的摘要（理论上）。
  * 隐藏状态 ht**h**t 就像状态机中的“当前状态”，它决定了模型如何响应当前输入，并影响未来的状态和输出。
* **状态转移机制** （状态的转移就是循环/递归过程)：

  - RNN 通过循环机制实现状态转移：当前隐藏状态 ht**h**t 依赖于前一时间步的隐藏状态 ht−1**h**t**−**1**** 和当前输入 xt**x**t。
  - 数学上，状态转移规则定义为：

    $$
    h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)
    $$

    - $ h_{t} \in \mathbb{R}^h $：当前时间步隐藏状态。
    - $ h_{t-1} \in \mathbb{R}^h $：前一时间步隐藏状态。
    - $ x_t \in \mathbb{R}^d $：当前输入。
    - $ W_{hh} \in \mathbb{R}^{h \times h} $：隐藏状态到隐藏状态的权重矩阵，用于传递历史信息。
    - $ W_{xh} \in \mathbb{R}^{h \times d} $：输入到隐藏状态的权重矩阵，用于处理当前输入。
    - $ b_h \in \mathbb{R}^h $：隐藏层的偏置向量。
    - $ f $：激活函数，通常为 tanh（值范围为 [-1, 1]）或 ReLU，用于引入非线性。

- **输出生成**：
  - 状态机在每个时间步可能产生输出，RNN 同样通过隐藏状态生成输出：

    $$
    y_t = W_{hy}h_t + b_y
    $$
  - 输出$y_t$可以看作状态机根据当前状态对外界的响应

**简单的数字识别卷积神经网络代码：**

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# 设置随机种子以保证结果可重复
torch.manual_seed(42)
if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

# 1. 加载和预处理 MNIST 数据集
# 定义数据预处理：将图像转换为张量并进行归一化
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # MNIST 图像均值为 0.5，标准差为 0.5
])

# 加载 MNIST 数据集 (训练集和测试集)
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# 创建数据加载器
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# 2. 定义 RNN 模型
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        # 定义 RNN 层：输入维度为 input_size，隐藏状态维度为 hidden_size，层数为 num_layers
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        # 定义全连接层：将 RNN 的最后一个时间步隐藏状态映射到输出类别
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # 输入 x 形状：[batch_size, 1, 28, 28]，将其重塑为 [batch_size, 28, 28]
        # 将图像按行看待为序列，28 个时间步，每个时间步输入 28 个像素值
        x = x.view(-1, 28, 28)  # 形状：[batch_size, seq_len, input_size] = [batch_size, 28, 28]
  
        # 初始化隐藏状态 h_0，形状：[num_layers, batch_size, hidden_size]
        batch_size = x.size(0)
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)
  
        # 前向传播 RNN
        out, hn = self.rnn(x, h0)  # out 形状：[batch_size, seq_len, hidden_size]
        # 取最后一个时间步的输出，形状：[batch_size, hidden_size]
        out = out[:, -1, :]
        # 通过全连接层得到分类结果，形状：[batch_size, output_size]
        out = self.fc(out)
        return out

# 3. 实例化模型、损失函数和优化器
input_size = 28       # 每个时间步输入 28 个像素（每行）
hidden_size = 128     # 隐藏状态维度
num_layers = 2        # RNN 层数
output_size = 10      # 输出类别数（0-9）
model = SimpleRNN(input_size, hidden_size, num_layers, output_size).to(device)
criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数，用于分类任务
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # SGD 优化器

# 4. 训练模型
num_epochs = 5
print("开始训练...")
for epoch in range(num_epochs):
    model.train()  # 设置模型为训练模式
    running_loss = 0.0
    for i, (images, labels) in enumerate(train_loader):
        images, labels = images.to(device), labels.to(device)
  
        # 前向传播
        outputs = model(images)
        loss = criterion(outputs, labels)
  
        # 反向传播和优化
        optimizer.zero_grad()  # 梯度清零
        loss.backward()  # 计算梯度
        optimizer.step()  # 更新参数
  
        running_loss += loss.item()
        if (i + 1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')
            running_loss = 0.0

# 5. 测试模型
model.eval()  # 设置模型为评估模式
correct = 0
total = 0
with torch.no_grad():  # 禁用梯度计算
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)  # 获取预测类别
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f'测试集准确率: {accuracy:.2f}%')

```

##### [长短期记忆网络](https://www.zhihu.com/tardis/zm/art/86006495?source_id=1003)

纯的RNN因为无法处理随着递归，权重指数级爆炸或[梯度消失问题](https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98 "梯度消失问题")，难以捕捉长期时间关联；而结合不同的**[LSTM](https://zh.wikipedia.org/wiki/LSTM "LSTM")**可以很好解决这个问题。

长短期记忆网络增加遗忘门、输入门及更新门、输出门来解决梯度问题以及长矩离依赖消失问题。

- C状态（单元状态)：主要负责长期记忆或长距离记忆，这是将以前的信息与当前输入的信息作按重要程度作了加权滤后的长期记忆
- h状态（隐藏状态)：主要负责短期记忆，包含当前时刻提炼后的信息，类似于当前注影力或工作记忆
- 解决长距离依赖主要依靠长期记忆的稳定性，C是对前一状态与当前输入的综合简选过滤后的信息，一定程度保证是综合而重要的语义信息

![1746239190760](image/1.机器学习与神经网络/1746239190760.png)

- LSTM 引入了记忆单元（cell state, $ c_t $）和三个门机制：

  - **遗忘门 (Forget Gate)**：决定丢弃哪些信息。

    $$
    f_t = \sigma(W_f[h_{t-1}, x_t] + b_f)
    $$
  - **输入门 (Input Gate)**：决定更新哪些新信息。

    $$
    i_t = \sigma(W_i[h_{t-1}, x_t] + b_i)
    $$

    $$
    \tilde{C}_t = \tanh(W_C[h_{t-1}, x_t] + b_C)
    $$
  - **更新记忆单元**：

    $$
    C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
    $$
  - **输出门 (Output Gate)**：决定输出哪些信息。

    $$
    o_t = \sigma(W_o[h_{t-1}, x_t] + b_o)
    $$

    $$
    h_t = o_t \cdot \tanh(C_t)
    $$

    $$
    y_t = W_{hy}h_t + b_y
    $$
- 数学上，LSTM 通过记忆单元 $ C_t $ 的加法更新（而非乘法），避免梯度消失，确保长距离信息传递。

##### 门控循环单元 (GRU)

- GRU 是 LSTM 的简化版本，使用更新门和重置门，减少参数量：

  - **更新门 (Update Gate)**：控制历史信息保留和新信息更新的比例。

    $$
    z_t = \sigma(W_z[h_{t-1}, x_t])
    $$
  - **重置门 (Reset Gate)**：决定遗忘多少历史信息。

    $$
    r_t = \sigma(W_r[h_{t-1}, x_t])
    $$
  - **隐藏状态更新**：

    $$
    \tilde{h}_t = \tanh(W[r_t \cdot h_{t-1}, x_t])
    $$

    $$
    h_t = (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t
    $$

    $$
    y_t = W_{hy}h_t + b_y
    $$
- GRU 参数更少，计算效率更高，但长距离依赖建模能力略弱于 LSTM。
- GRU 取消长期记忆，但将h作为综合记忆，在RNN基础上加上对原有记忆（即上一状态)及输入作为过滤，以区分信息的权重

### 全局关联语义模型

##### Transformer

参考文档：

- [Transformer模型详解](https://zhuanlan.zhihu.com/p/338817680)
- [深入浅出词Word2Vec原理](https://zhuanlan.zhihu.com/p/114538417)
- [Seq2Seq：Transformer](https://medium.com/@x02018991/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98-transformer-628cfcd17ba6)
- [深度学习attention机制中的Q,K,V分别是从哪来的？](https://www.zhihu.com/question/325839123)
- [NLP中的Attention注意力机制+Transformer详解](https://zhuanlan.zhihu.com/p/53682800)

###### Seq2Seq模型

神经网络的结构决定了整个模型是$N \rightarrow M$的映射，即固定输入N个元素，固定输出M个元素。

然而现实中更多的场景是输入是一个不确定长度的序列，而输出也是一个不确定长度的序列，这种场景最普遍是的人类使用文字或语音进行交流时，输入一段话，经过神经网络处理后，再输出一段话，以达成翻译、对话等应用目的。

Seq2Seq神经网络模型就是针对此应用而设计。

![1746495356051](image/1.机器学习与神经网络/1746495356051.png)

Seq2Seq将任务分成两部分：

- 输入序列的理解：由Encoder完成，目的是将输入序列作全局性的语义理解，类似于人的听/看及思考的过程，在神经网络中将输入序列的全局语义转换为高维嵌向量
- 输出序列的生成：由Decoder完成，目的是根据全局性的语义，生成相对应的输出序列，类似于人的说/写的过程，在神经网络中根据高维嵌向量生成具体序列

###### Seq2Seq语义原理

![1746497507070](image/1.机器学习与神经网络/1746497507070.png)

Seq2Seq的核心在于如何从一个序列中获取全局性的综合的语义信息，其核心原理是语义通过关系来表现，比如一个词Cat，按人的理解是有一个实物与这个词Cat对应，Cat的语义是通过实物的性质获得。但在神经网络中缺乏实体语义条件，因此一个词的语义是通过与其它词的相互表达或关系来实现的。比如Cat在大量文章中，会发现Cat与"动物/可爱/老鼠"大量相关，于是神经网络可以据此来理解Cat这个词的语义。

因此Seq2Seq Encoder的语义理解需要遍历所有输入的词，并且将这些词的关系找出并整理，最终形成包含所有信息的综合语义向量，从而实现一个词乃至一段话（一个序列的理解）。

Decoder是一个序列的生成，其核心原理是提据提示词，查询Encoder生的语义空间中的向量，使生成的输出与输入序列是基于同样的语义空间，从而Decoder的输出就是输入序列的相关的翻译或解答。

Decoder可以是一个具体的神经网络结构，比如上图的对话图结构，但Decoder也可以是一个隐式的结构，比如只输出简间的分类，即可以不用显示的Decoder结构，隐式的Decoder结构（或者说Decoder逻辑）仍然存在。

基于全局关联语意的Transformer模型就是一个基于显式Encoder-Decoder的神经网络模型，比起RNN/CNN，会有强的能力及更复杂的应用场景。

###### Transformer历史

![1746498203387](image/1.机器学习与神经网络/1746498203387.png)

2017 年 Google 发布[《Attention is All You Need》](https://arxiv.org/abs/1706.03762)之后，各种基于 Transformer 的模型和方法层出不穷。尤其是 2018 年，OpenAI 发布的 [GPT](https://openai.com/blog/language-unsupervised/) 和 Google 发布的 [BERT](https://aclanthology.org/N19-1423/) 模型在几乎所有 NLP 任务上都取得了远超先前最强基准的性能，将 Transformer 模型的热度推上了新的高峰。

Transformer 模型之所以如此强大，是因为它抛弃了之前广泛采用的循环网络和卷积网络，而采用了一种特殊的结构——注意力机制 (Attention) 来建模文本，注意力机制对于输入序列的理解能力会比CNN/RNN更准确而深入。

###### Transformer的哲学原理

Transformer背后的基础是两个影响深远的哲学原理，这两个哲学原理让基于Transformer构建的大语言模型具备强大的能力：

- 西方的结构主义/东方的缘起理论（关系主义）：任何一个事件都不是能够独立存在的个体，而是由其它因素通过一定的关系建构而成。这是Transformer的学习及理解的哲学原理，也就是注意力机制的哲学原理。
- 维特根斯坦的《逻辑哲学论》：“对于不可说的东西我们必须保持沉默。”这是大语言模型的知识建构的哲学原理，意即：所有的知识都包含在可以言说的语言文字之中，所以通过大量的文本，可以获得近忽所有的知识。

###### Transformer整体结构

https://zhuanlan.zhihu.com/p/338817680：Transformer模型详解

https://transformers.run/c1/attention : Transformer快速入门

https://zh.d2l.ai/index.html: 动手学深度学习

![1744945228833](image/1.机器学习与神经网络/1744945228833.png)

Transformer由Encoder及Decoderf组成，其中Encoder完成输入序列的全局语义理解，并将此语义作为嵌向量Q/K输出，Q/K提供一种关系。

Transformer的Encoder及Decoder分成几部分：

- 输入序列编码部分：
  - **输入用嵌入(Input Embedding)**
  - **位置编码(Positional Encoding)**
- 语义特征提取部分：这部分是Transformer的核心算法，包括以下有关注意力的部分：
  - 用于Encoder的自注意力机制：自注意力机制实现输入语意的理解
  - 用于并行计算的多头注意力机制：提取多维的注意力语义，并加速注意力计算过程
  - 用于Decoder的交叉注意力机制：用于Decoder使用Encoder语义空间逐渐生成新的token序列
  - 用于Decoder的掩码注意力机制：只关注前向的输入序列
- 神经模型建模/拟合部分：
  - **前馈神经网络(Feed Forward)** 实现神经网络模型拟合
  - 残差网络及归一化(Add & Norm)：是神经网络的训练技巧
- 输出序列部分：
  - 线性层+Softmax：实现输出的概率均一化。

###### [Encoder/Decoder词嵌入：](https://www.cnblogs.com/rossiXYZ/p/18741857)

[输入嵌入](https://www.cnblogs.com/rossiXYZ/p/18741857)部分是将输入文本拆分成为比单词更基础的TOKEN，并将此TOKEN向量化的过程，向量化后的TOKEN是数值化的矩阵，包含有此TOKEN的抽象化的词义信息。

1. 根据输入建立词汇表（基于TOKEN，而不是整个单词），使用词汇表中的索引来表示具体的某个token
2. 建立 大小为$V \times d$嵌入矩阵，$V$表代词汇表数量，$d$代表嵌入矩阵的维数（一个词义的特征数），嵌入矩阵的每一行对应一个符号的嵌入向量
3. 嵌入向量维度$d$是一个超参数，通常为512或768，嵌入向量的参数的一部分，可以使用随机初始化训练，也可以使用预训练的Word2Vec实现
4. 词向量代表一个词独立于上下文的全局词意信息

![1746611833454](image/1.机器学习与神经网络/1746611833454.png)

这里，词向量实现最初基于词义的理解：每个词的词意通过高维向量来表达。

词向量的意义特征：

- 词向量是单词词义的数学化及结构性的表达，其目的为了使用矩阵作神经网络计算
- 词向量的维度可以认为是各个不同的特征维，代表一种抽象的词义
- 词义/特征的抽取或组合，或构造另一词向量
- 词与词之间关系可以通过向量之间的范数矩离来计算

  ![1746670702121](image/1.机器学习与神经网络/1746670702121.png)

  距离相近的词代表有类似的语义，基于某组特征能分为同一类别

  ![1746669030820](image/1.机器学习与神经网络/1746669030820.png)
- 词与词之间的向量向法/减法可以实现

  ![1746669044174](image/1.机器学习与神经网络/1746669044174.png)
- 词向量是整个Transformer（或其它处理语言的神经网络）的基础

  - 词向量将词义作数值化编码
  - 词向量将词作特征提取及特征分解
  - 基于向量最核心的作用可以用数学运算来表达词与词之间的关系（关系是Transformer最核心的思想)
  - 基于向量可以通过线性变换作不同的数值及维度变换，比如可以降维、分类等运算

从Token到词向量是一个复杂的过程，需要进行编码转换及特征分析，所以也是由神经网络实现的，常见有两类：

* 直接使用预训练好的词向量且对词向量不做改变。例如可以采用 Word2Vec、Glove 等算法预训练得到。因为在使用模型时（推理阶段），这些嵌入向量会作为输入送入模型的后续层进行处理，不会发生变化，因此在这种情况下，嵌入矩阵实际就是一个查找表。如果监督数据较少，我们可以固定Embedding，只让模型学习其它的参数。这也可以看成一种 Transfer Learning。
* 使用动态词向量。所谓动态词向量就是在训练一个特定任务的同时，同时也基于语料来训练词向量了，Embedding矩阵就是模型训练后的产出。具体来说就是Transformer 中训练得到一个embedding层。每个token在Embedding层中都有一个对应的高维表示，这种映射关系在模型训练的过程中一直在更新，不断进行改进。

以下是Transformer的嵌入层的实现：

```python
class InputEmbeddings(nn.Module):

    def __init__(self, d_model: int, vocab_size: int) -> None:
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, d_model)

    def forward(self, x):
        # (batch, seq_len) --> (batch, seq_len, d_model)
        # Multiply by sqrt(d_model) to scale the embeddings according to the paper
        return self.embedding(x) * math.sqrt(self.d_model)
```

###### [Encoder/Decoder位置编码](https://www.cnblogs.com/rossiXYZ/p/18744797)：

作为Transformer的基本处理语言元素的Token，在实现输入嵌入后，词向量包括该单词的基本词意信息（此词意信息是基本输文本单词之间的关系获取），但一个词的语义是和此词在语子中的位置关系严格相关的，词的在句子中的语义信息由以下组成：

- 词本身的涵义：由词向量提供
- 词的成分信息，如作为主语、谓语、宾语、定语、状语等信息，由词在句子中的相对位置提供
- 词的语义信义，由词与词之间的线性结构关系统提供，如“从北京去上海"与“从上海去北京"，语义完全不一样

位置编码可以作为词义特征之外的单独特征，可以集成在嵌入向量编码中学习，也可以将其分开作为独立的编码，位置编码需要考虑以下因素：

- 唯一性：每个位置的位置编码是唯一的，不论句子长度如何变化，同一位置应该有同样的编码
- 相对性：对模型来说，真正重要的往往不是绝对位置，而是token之间的相对位置。有此词的语义与相对位置有关，则位置编码是特定的位置，但有的词无论在什么位置其意义相同，那么此词的位置编码可以相同的。
- 距离衰减性：位置编码的最大意义就是给模型提供位置语义相关性。而位置相关性应该随相对位置距离增大而减少，并且是单调衰减关系。具体就是距离近的相关性高，距离远的相关性低。
- 平移不变性：任何位置之间的相对距离在不同长度的句子中应该是一致的。具体来说是，两个位置的关系只与相对位置有关，与序列长度无关。在任何长度不同的序列中，相同位置的Token之间的相对位置/距离保持一致
- 线性关系。位置之间的关系在数学上应该是简单的，或者说存在线性关系。
- 结合语义信息。在涉及长上下文理解和搜索的任务中，注意力机制应该优先考虑语义相似性，而不是被与位置编码相关的信息所掩盖，因为在较长距离上位置编码的相关性可能较低。

Transformer不像RNN天然具备串行的先后顺序，所以为每个Token附加额外的位置信息，Token是一个向量，可以在此向量作加法加上位置编码。

![1746672531289](image/1.机器学习与神经网络/1746672531289.png)

Transformer的位置信息用不同的频点来表示，使用波型的迭加原理将位置频点加上词向量后，就得到了具备位置信息的词向量，此词向包含置信息及基本的词义信息。

$$
PE_{pos,2i}=\sin(\frac{pos}{10000^{2i/d}})
$$

$$
PE_{pos,2i+1}=\cos(\frac{pos}{10000^{2i/d}})
$$

![1746674214271](image/1.机器学习与神经网络/1746674214271.png)

```python
class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:
        super().__init__()
        self.d_model = d_model
        self.seq_len = seq_len
        self.dropout = nn.Dropout(dropout)
        # Create a matrix of shape (seq_len, d_model)
        pe = torch.zeros(seq_len, d_model)
        # Create a vector of shape (seq_len)
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)
        # Create a vector of shape (d_model)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)
        # Apply sine to even indices
        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))
        # Apply cosine to odd indices
        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))
        # Add a batch dimension to the positional encoding
        pe = pe.unsqueeze(0) # (1, seq_len, d_model)
        # Register the positional encoding as a buffer
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)
        return self.dropout(x)
```

###### [Encoder的QKV自注意力机制](https://www.cnblogs.com/rossiXYZ/p/18751758)：

Transformer的核心算法在序列的全局语意理解过程中，使用QKV注意力机制实现语意**在理解层面的分布化**，以及**在计算层面的并行化**。

在解释QKV注意力机看之前，需要说明一个背景知识：对于神经网络而言，由先在结构上设计了某种机制（比如QKV机制或卷积机制)，但是由于参数内容是由神对网络生成的，所以这些结构里的参数的具体涵义，实际上是很难完全确定的，这就是神经网络具有不可解释性。我们只能大致理解或类比理解，不能够象程序算法那样准确地描述或理解。

![1746684514145](image/1.机器学习与神经网络/1746684514145.png)

QKV注意力机制使用三个参数矩阵实现上下文语意（相对于词向量的词义为高层语义）的表示与存储，即Query，Key，Value参数矩阵，分别用$W_Q, W_K, W_V$来表示，这三个矩阵由模型在训练时生成，训练完成后$W_Q, W_K, W_V$代表了整个训练数据的语义上下文空间，也就是说，训练后整个训练文本的语意信义被分布式的存储在高维的参数矩阵$W_Q, W_K, W_V$中。

其中Query提供词向量与其它词的相关度，Value为词向量本身的语义信息，Key为词向量Value的索引。

当一个序列中的元素（比如一个词）要计算它自己的新表示时，它会同时扮演三个角色，通过不同的线性变换（乘以$W_Q, W_K, W_V$矩阵）从其原始嵌入中派生出Q, K, V：

* 作为提问者 **(Q)** ：它想知道序列中其他（包括自己）哪些元素与它当前关注的方面最相关。
* 作为被索引者 **(K)** ：它提供一个“可被查询”的特征，供其他元素的Q来匹配。
* 作为信息提供者 **(V)** ：它包含该元素实际的语义信息，一旦被Q-K匹配选中，它的信息就会被传递出去。

其中Query提供词向量与其它词的相关度，Value为词向量本身的语义信息，Key为词向量Value的索引。

设$X$为n个输入的嵌入词向量矩阵，每一行代表一个带位置信息的输入词向量，那么注意力机制是只关心词向量在具体上下文中的语义，比如“苹果"可以有”手机/水果/..."等语意，那么在上下文“我用苹果打电话"里，”苹果"的语义就会被理解成为“手机"而不是“水果"。

QKV自注意力机意可以用[数据库解释]()：一个词或一段话的语意信义通过$K,V$来表示，可以用数据库的表来作类比：$K$代表数据的索引表，而$V$代表具体的数据内容。而$Q$是查询语句（或查询词），我们要想了解一个词"Apple"在上下文空间的特定涵义，那么先用"Apple"去查询输入上下文语句“I have a Apple phone"中每个词的索引表，再由索引表获取对应词的涵义，因此"Apple"在此上下文中，其语义会偏向于是"手机"而非"水果"。所以Transformer的原理可以解释为：用输入的提示词去查询全局语义数据库，返回查询后的语义并根据要求解码成想要的输出。

QKV自注意力机意也可以[搜索引擎]()解释：一个输入的句子是搜索的关键字或一句话，而返回的$K$是搜索引擎返回的条目，$V$则是每个条目的具体内容。所以Transformer可以认为是一个基于语义空间的高级搜索引擎，而一般人使用大语言模型也与搜索引擎类似。

使用自注意力机制可以计算出每个词在输入上下文中的语义，也就是每个词都包含有全局上下文的信息，可以通过以下方式计算出一个词向量$X$在上下文中的注意力语义$X_{Attention}$：

$$
Q_X=XW_Q, K_X=XW_K, V_X=XW_K
$$

$$
X_{\mathrm{Attention}}=\mathrm{softmax}(\frac{Q_XK^T_X}{\sqrt{d_k}})V_X
$$

其中$Q_X,K_X,V_X$为具体词的QKV参数，而$Q_X K^T_X$为查询Q与索引K的点积，**向量的点积**代表元素与元素（这里是词与词)的相关度（或关注度)，这里的相关度就是**注意力**。$X_{\mathrm{Attention}}$就是参考了与其它词的相关度（或注意力)的、在此上下文中的语义。

![1746685139734](image/1.机器学习与神经网络/1746685139734.png)

![1746685608824](image/1.机器学习与神经网络/1746685608824.png)

因为$X_{Attention}$中的每行的词向量都包含全局上下文语意信息，所以全局语意信息是分布在每个向量而不是由某一个单一的向量进行综合，即每个词里面实际上都包含有部分全局语意，这就实现了全局语义的分布化表示，在这种情况下，去掉具体某一个词并不会对全局上下文语意造成太大影响，反而可以根据综合其它词的语意对缺失的词进行语意补全（这是BERT的原理)。

![1746685231020](image/1.机器学习与神经网络/1746685231020.png)

在Encoder中，$Q$与$K,V$都来源于同样的输入$X$，所以叫自注意力，自注意力是Transformer的核心，因为模型上下文语意是由自注意力机制构建的。

$Q$可以来源于$Y$，而$K,V$都来于同样的输入$X$(需要注意$K,V$的来源必需一样)，所以叫交叉注意力，Decoder中的第二层就是交叉注意力。

###### Encoder/Decoder[多头注意力](https://www.cnblogs.com/rossiXYZ/p/18759167)（Multi-Head Attention）

多头注意力MHA是Transformer的核心机制之一，它将QKV投影到多个子空间（子空间的数量即头数$h$，子空间维数为$d/h$)。

![1746609331923](image/1.机器学习与神经网络/1746609331923.png)

多头注意力的引入解决两个问题：一是并行计算，二是多头注意力可以从更多的维度获取上下文相关信息，从而得到更完整更深层次的上下文信息。

多头注意力的语义原理是一个句子的语义是多层次的，比如一句话，可以从“事实层面”、“字词韵律层面”、“语气层面“等作相关分析，用多头注意力可以解决这个问题，每一个单头如可以专注于某一特征的语义信息，然后再进行合并综合，就得到了多层次全面综合的语意信息。

多头注意力机制的流程：

- 将$Q,K,V$通过不同的线性变换影到不同的子空间：$Q_i=QW_Q,K_i=KW_K,V_i=VW_V$，这里$W_Q,W_K,W_V$是投影矩阵
- 计算多头注意力：$\mathrm{Head}_i=\mathrm{Attention}(Q_i,K_i,V_i)=\mathrm{Softmax}(\frac{Q_iK^T}{\sqrt{d_k}K})V_i$
- 拼接多头注意力并通过线性变换矩阵$W_o$映射回原始维度：$MultiHead(Q,K,V)=Concat(head_1,head_2,...,head_h)W_o$

![1746685548722](image/1.机器学习与神经网络/1746685548722.png)

```python
class MultiHeadAttentionBlock(nn.Module):

    def __init__(self, d_model: int, h: int, dropout: float) -> None:
        super().__init__()
        self.d_model = d_model # Embedding vector size
        self.h = h # Number of heads
        # Make sure d_model is divisible by h
        assert d_model % h == 0, "d_model is not divisible by h"

        self.d_k = d_model // h # Dimension of vector seen by each head
        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq
        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk
        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv
        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo
        self.dropout = nn.Dropout(dropout)

    @staticmethod
    def attention(query, key, value, mask, dropout: nn.Dropout):
        d_k = query.shape[-1]
        # Just apply the formula from the paper
        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)
        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            # Write a very low value (indicating -inf) to the positions where mask == 0
            attention_scores.masked_fill_(mask == 0, -1e9)
        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax
        if dropout is not None:
            attention_scores = dropout(attention_scores)
        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)
        # return attention scores which can be used for visualization
        return (attention_scores @ value), attention_scores

    def forward(self, q, k, v, mask):
        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)
        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)
        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)

        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)
        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)
        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)
        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)

        # Calculate attention
        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)
  
        # Combine all the heads together
        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)
        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)

        # Multiply by Wo
        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  
        return self.w_o(x)
```

###### Encoder/Decoder[残差网络及归一化](https://www.cnblogs.com/rossiXYZ/p/18774865)

![1746603709977](image/1.机器学习与神经网络/1746603709977.png)

Transformer需要多层次的训练，一般神经网络的层数越多，越能学习到复杂的特征，越能提升模型的性能。但是，事实上，当层数增加到一定程度之后，模型的性能会急剧下降。导致上述情形的主要是三个问题：

- 梯度消失（Vanishing Gradient）：梯度消失是指在训练深度神经网络时，反向传播过程中的梯度逐渐变小，最终导致梯度接近于零的现象。
- 梯度爆炸（gradient exploding）：梯度爆炸指的是在反向传播过程中，梯度值呈指数级增长，导致网络权重的大幅更新，从而使得网络变得不稳定，训练效果下降。
- 网络退化：理论上网络存在一个最优层数，如果模型超过这个层数，则多余层数带来的效果并不超过该最优层数下的模型效果，即这些冗余层数会带来网络退化。

清华的何凯明提出的残差连接（Residual connection/skip-connect）可以有效缓解这些这些问题。从而使得拥有几十上百层的深度学习模型更加容易训练，在增加模型深度的同时还能保持甚至提高准确度。

残差的数学表达式：$y=F(x)+x$，即$F(x)=y-x$，模型训练一是输出与输入之间的差异，所以叫残差网络

![1746693002190](image/1.机器学习与神经网络/1746693002190.png)

Transformer在每一功能层都会增加输入的残差并作归一化处理，以避免由于层数深而导致的性能退化，以及优化梯度计算。

自注意力层的残差连接（采用Pre-LN)：

$$
\mathrm{Output}_\mathrm{Attention}=\mathrm{Norm}(\mathrm{MultiHead}(Q,K,V))+ \mathrm{Input}
$$

前馈神经网络的残差连接：

$$
\mathrm{Output}_\mathrm{FFN}=\mathrm{FFN}(\mathrm{Input})+ \mathrm{Input}
$$

```python
class ResidualConnection(nn.Module):
  
        def __init__(self, features: int, dropout: float) -> None:
            super().__init__()
            self.dropout = nn.Dropout(dropout)
            self.norm = LayerNormalization(features)
  
        def forward(self, x, sublayer):
            return x + self.dropout(sublayer(self.norm(x)))
```

[归一化](https://zhuanlan.zhihu.com/p/424518359)是为了解决输入数据分布差异太大，会导致梯度下降算法的迭代步数以及梯度更新的难度，从而影响训练的收敛性。

归一化把输入数据X，在输送给神经元之前先对其进行平移和伸缩变换，将X的分布规范化成在固定区间范围的标准分布，简单的说就是控制每一层输入数据（激活值）的分布，把一部分不重要的复杂信息过滤掉，将数据拉回标准正态分布，使得数据的分布稳定下来（降低各维度数据的方差，减少不同层之间的输入分布变化，尽可能让输入的数据/特征变得独立同分布），以此来降低拟合难度和过拟合的风险，加速模型的收敛。

![1746694677740](image/1.机器学习与神经网络/1746694677740.png)

$$
\hat{x} = \alpha\frac{x - \mu}{\sigma+\epsilon }+b
$$

其中

* $\mu$ 和 $\sigma$是样本$L$ 的均值和方差，$\epsilon$是一个小常数，用于防止除零错误。
* $a,b$ 可学习参数

```python
class LayerNormalization(nn.Module):

    def __init__(self, features: int, eps:float=10**-6) -> None:
        super().__init__()
        self.eps = eps
        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter
        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter

    def forward(self, x):
        # x: (batch, seq_len, hidden_size)
         # Keep the dimension for broadcasting
        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)
        # Keep the dimension for broadcasting
        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)
        # eps is to prevent dividing by zero or when std is very small
        return self.alpha * (x - mean) / (std + self.eps) + self.bias
```

###### Encoder[前馈神经网络FFN](https://www.cnblogs.com/rossiXYZ/p/18765884)

![1746694803721](image/1.机器学习与神经网络/1746694803721.png)

前馈神经网络是所有不同类型神经网络的核心，通过前馈神经网络才能建立映射或拟合模型，对于Transformer而言，输入嵌入及QKV多头注意力理论上是进行数据的特征处理，FFN才是建立输入到输出的映射模型。FFN的主要作用：

* 提取并加工更完整及更深层次的语义信息：QKV注意力理论上的词义信息，深层语意信息需要深层全连接网络实现。
* 增加表达能力：多层全连接神经网络可以作不同形式的映射，映射即是表达。
* 存储知识：FFN网络的参数矩阵存储了全局语义信息

$$
X_\mathrm{output}=\mathrm{FFN}(X_\mathrm{Attention})=\mathrm{ReLU}(X_\mathrm{Attention}W_1+b_1)W_2+b_2
$$

```python
class FeedForwardBlock(nn.Module):

    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:
        super().__init__()
        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2

    def forward(self, x):
        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model), 512->2048->512
        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))
```

###### Decoder[掩码注意力](https://www.cnblogs.com/rossiXYZ/p/18758992)

机器学习领域中，掩码（Mask）本质是一个跟需要掩盖的目标张量大小一致的（大多数是0-1二值）张量，其思想最早起源于 word2vec 的CBOW的训练机制：通过上下文来预测中心词。掩码就相当于把中心词给遮掩住。不同的任务和应用场景可能需要不同类型的mask操作。在自注意力模型中，常见的mask操作有两种：Padding mask和Sequence mask。

- Padding mask（填充掩码）：在处理变长序列时，为了保持序列的长度一致，通常会在序列的末尾添加一些特殊的填充符号（如）。Padding mask的作用是将这些填充符号对应位置的注意力分数设为一个很小的值（如负无穷），从而使模型在计算注意力分数时忽略这些填充符号，避免填充符号对计算产生干扰。
- Sequence mask（序列掩码）：在某些任务中，为了避免模型在生成序列时看到未来的信息，需要对注意力分数进行掩码操作。Sequence mask的作用是通过构建下三角（或者上三角）的注意力分数矩阵，将当前位置之后位置的注意力分数设为一个很小的值，从而使模型只关注当前 token 与之前 token 的注意力关系，不理会它与后续 token 的关系。这样可以保证模型在生成序列时只依赖于已经生成的部分，不会受到未来信息的影响，即只”看”当前及前面的 tokens。也有把Sequence mask叫做Casual Mask的。

编码器和解码器的运行方式不同：

* Encoder因为要编码整个句子，每个词都需要考虑上下文的关系。所以每个词在计算的过程中都是可以看到句子中所有词的。
* 但是Decoder实质上是一个单向的自注意力结构，每个词都只能看到前面词的状态。原因如下：推理阶段是自回归模式，是一个词一个词输入的，Decoder是不知道下文信息的。所以每次decoder只能看到之前自己生成的token和prompt，因此自然也无法计算得到当前词和下文还没出现词的注意力。

解码器这种运行方式导致其在训练时候需要做特殊处理。因为训练阶段采用自回归模式，会导致训练速度过慢。如前文所述，为了加快训练速度，人们采用了Teacher Foring。即采用类似编码器中的矩阵并行算法，一步就把所有目标单词预测出来。这样做有两个好处，一是通过多样本并行计算能够加快网络的训练速度；二是在训练过程中直接喂入解码器正确的结果而不是上一时刻的预测值（因为训练时上一时刻的预测值可能是错误的），可以让训练更快收敛。

![1746756106303](image/1.机器学习与神经网络/1746756106303.png)

所以每个输入的词只会关心此词之前的序列的语义，因此在生成Decoder输入的序列的综合语意时，每个多头注意力会加上一个序列掩码矩矩$M$

对于长度为$n$的序列，构建一个$n \times n$的上三角矩阵$\textbf{M}$，其中位于右上半部分（不包括对角线）的元素被设为负无穷（通常使用一个非常小的数，如$-1e9$），而左下半部分（包括对角线）则保持为0。

$$
\textbf{M}_{ij} =
\begin{cases}
0, & \text{if } j \leq i \\
-\infty, & \text{otherwise}
\end{cases}
$$

$$
X_{\mathrm{Attention}}=\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}}+M)V
$$

这样做之后，那些被设置为负无穷的位置将会转换为0，从而有效地“遮蔽”了未来的信息。即$\mathrm{X}_{\mathrm{Attention}}$中的每一行都只包含当前Token之前的语义信息，使用此语意信息去查询Encoder的全局语意空间，就可以生成每行的预测输出（每行的预测输出也只包含此tocker之前序列的语意)。

| 变量名称 | mask类型                     | 编码器Self-attention | 解码器masked self-attention             | 解码器Cross-attention |
| -------- | ---------------------------- | -------------------- | --------------------------------------- | --------------------- |
| src_mask | Padding Mask                 | 使用                 | 不使用（padding的功能在tgt_mask中完成） | 使用                  |
| tgt_mask | Padding Mask + Sequence Mask | 不使用               | 使用                                    | 不使用                |

```python
def get_or_build_tokenizer(config, ds, lang):
    tokenizer_path = Path(config['tokenizer_file'].format(lang))
    if not Path.exists(tokenizer_path):
        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour
        tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))
        tokenizer.pre_tokenizer = Whitespace()
        trainer = WordLevelTrainer(special_tokens=["[UNK]", "[PAD]", "[SOS]", "[EOS]"], min_frequency=2)
        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)
        tokenizer.save(str(tokenizer_path))
    else:
        tokenizer = Tokenizer.from_file(str(tokenizer_path))
    return tokenizer

class MultiHeadAttentionBlock(nn.Module):

    @staticmethod
    def attention(query, key, value, mask, dropout: nn.Dropout):
        d_k = query.shape[-1]
        # Just apply the formula from the paper
        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)
        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            # Write a very low value (indicating -inf) to the positions where mask == 0
            attention_scores.masked_fill_(mask == 0, -1e9)
        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax
        if dropout is not None:
            attention_scores = dropout(attention_scores)
        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)
        # return attention scores which can be used for visualization
        return (attention_scores @ value), attention_scores

class DecoderBlock(nn.Module):

    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.cross_attention_block = cross_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))
        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))
        x = self.residual_connections[2](x, self.feed_forward_block)
        return x
  
```

###### Decoder的交叉注意力

Transformer模型或其它神经网络模型从学习的原理而言，分为两个部分：

- 学习/理解（Encoder)
  - 前期系统学习理解：如同基础教育，建构整个知识体系，类似于Transformer通过大量的语料进行训练，建立全局知识体系统的语义数据库
  - 针对具体提示词的理解：在推理过程中，由Encoder将输人语句在全局知识语义数据库中作上下文语义理解，建立上下文语义数据库
- 输出/转换(Decoder)：由输出提示词查询语义数据库，逐步获取查询输出，获得整个语义输出，并转换成人类的文或其它能理解的形式

![1746760352218](image/1.机器学习与神经网络/1746760352218.png)

Decoder经过掩码注意力后得到的综合语义，会作为Decoder交叉注意力的Q的输入，而Encoder输出的综合意义，会作为Decoder交叉注意力的K/V的输入，其作用是使用Decoder的输入去获得Encoder语义空间的综合语义输出。

与Encoder的自注意力及Decoder的自注意力不同的是，交叉注意力的Q与K/V不是来是同一输入，所以叫交叉注意力。

$$
X_{\mathrm{CrossAttention}}=\mathrm{softmax}(\frac{Q_DK_E^T}{\sqrt{d_k}})V_E
$$

经过多头交叉注意力后，得到最终根据预训练的全局综合语义(1)+Encoder输入问题的综合语义(2)+Decoder引导词的综合语义(3)的汇总语义，建输出语义，交由前馈神经网络建立拟合模型。

这里可以用人类的学习与认知作对比：人类要回答一个问题，首先是需要学习此问题的相关领域知识（即训练时生成的综合语义1），然后要对具体的问题进行分析理解（即Encoder输入问题的综合语义2），然后试图顺序输出回复，回复的过程会根据之前的语句作参考（即Decoder的综合语义3），通过此过程直接完成问题的回答，这和Transformer的注意力机制是相同的。

```python
class DecoderBlock(nn.Module):

    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.cross_attention_block = cross_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))
        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))
        x = self.residual_connections[2](x, self.feed_forward_block)
        return x
```

###### Decoder前馈神经网络FFN

Decoder的FFN与Encoder的FFN原理与作用完全一样。

前馈神经网络是所有不同类型神经网络的核心，通过前馈神经网络才能建立映射或拟合模型，对于Transformer而言，输入嵌入及QKV多头注意力理论上是进行数据的特征处理，FFN才是建立输入到输出的映射模型。

$$
X_\mathrm{output}=\mathrm{FFN}(X_\mathrm{Attention})=\mathrm{ReLU}(X_\mathrm{Attention}W_1+b_1)W_2+b_2
$$

###### Decoder最后的线性映射与Softmax

![1746760476426](image/1.机器学习与神经网络/1746760476426.png)

在 Transformer 模型中，**Decoder 的最后一步**是生成目标序列的预测结果。这一过程通常包括两个关键步骤：

* **线性映射（Linear Projection）**：将 Decoder 的输出（通常是多头的注意力机制和前馈神经网络的组合结果）映射到目标词汇表的维度。
  * 理解映射：映射是将内容/信息用不同方式表现，这是线性代数及神经网络的核心思想
* **Softmax**：将线性映射的结果转换为概率分布，选择概率最高的 token 作为预测结果（在训练时，通常使用交叉熵损失函数）

![1746760863459](image/1.机器学习与神经网络/1746760863459.png)

```python
class ProjectionLayer(nn.Module):

    def __init__(self, d_model, vocab_size) -> None:
        super().__init__()
        self.proj = nn.Linear(d_model, vocab_size)

    def forward(self, x) -> None:
        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)
        return self.proj(x)
```

###### [Transformer的代码实现](http://nlp.seas.harvard.edu/annotated-transformer/)

[理解Transformer模型1：编写Transformer](https://zlotus.github.io/2024/01/06/transformer-from-scratch-1/)

[理解Transformer模型2：训练Transformer](https://zlotus.github.io/2024/01/13/transformer-from-scratch-2/)

```python
import torch
import torch.nn as nn
import math

class LayerNormalization(nn.Module):

    def __init__(self, features: int, eps:float=10**-6) -> None:
        super().__init__()
        self.eps = eps
        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter
        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter

    def forward(self, x):
        # x: (batch, seq_len, hidden_size)
         # Keep the dimension for broadcasting
        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)
        # Keep the dimension for broadcasting
        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)
        # eps is to prevent dividing by zero or when std is very small
        return self.alpha * (x - mean) / (std + self.eps) + self.bias

class FeedForwardBlock(nn.Module):

    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:
        super().__init__()
        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2

    def forward(self, x):
        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)
        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))

class InputEmbeddings(nn.Module):

    def __init__(self, d_model: int, vocab_size: int) -> None:
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, d_model)

    def forward(self, x):
        # (batch, seq_len) --> (batch, seq_len, d_model)
        # Multiply by sqrt(d_model) to scale the embeddings according to the paper
        return self.embedding(x) * math.sqrt(self.d_model)
  
class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:
        super().__init__()
        self.d_model = d_model
        self.seq_len = seq_len
        self.dropout = nn.Dropout(dropout)
        # Create a matrix of shape (seq_len, d_model)
        pe = torch.zeros(seq_len, d_model)
        # Create a vector of shape (seq_len)
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)
        # Create a vector of shape (d_model)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)
        # Apply sine to even indices
        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))
        # Apply cosine to odd indices
        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))
        # Add a batch dimension to the positional encoding
        pe = pe.unsqueeze(0) # (1, seq_len, d_model)
        # Register the positional encoding as a buffer
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)
        return self.dropout(x)

class ResidualConnection(nn.Module):
  
        def __init__(self, features: int, dropout: float) -> None:
            super().__init__()
            self.dropout = nn.Dropout(dropout)
            self.norm = LayerNormalization(features)
  
        def forward(self, x, sublayer):
            return x + self.dropout(sublayer(self.norm(x)))

class MultiHeadAttentionBlock(nn.Module):

    def __init__(self, d_model: int, h: int, dropout: float) -> None:
        super().__init__()
        self.d_model = d_model # Embedding vector size
        self.h = h # Number of heads
        # Make sure d_model is divisible by h
        assert d_model % h == 0, "d_model is not divisible by h"

        self.d_k = d_model // h # Dimension of vector seen by each head
        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq
        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk
        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv
        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo
        self.dropout = nn.Dropout(dropout)

    @staticmethod
    def attention(query, key, value, mask, dropout: nn.Dropout):
        d_k = query.shape[-1]
        # Just apply the formula from the paper
        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)
        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            # Write a very low value (indicating -inf) to the positions where mask == 0
            attention_scores.masked_fill_(mask == 0, -1e9)
        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax
        if dropout is not None:
            attention_scores = dropout(attention_scores)
        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)
        # return attention scores which can be used for visualization
        return (attention_scores @ value), attention_scores

    def forward(self, q, k, v, mask):
        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)
        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)
        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)

        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)
        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)
        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)
        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)

        # Calculate attention
        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)
  
        # Combine all the heads together
        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)
        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)

        # Multiply by Wo
        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  
        return self.w_o(x)

class EncoderBlock(nn.Module):

    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])

    def forward(self, x, src_mask):
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))
        x = self.residual_connections[1](x, self.feed_forward_block)
        return x
  
class Encoder(nn.Module):

    def __init__(self, features: int, layers: nn.ModuleList) -> None:
        super().__init__()
        self.layers = layers
        self.norm = LayerNormalization(features)

    def forward(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)

class DecoderBlock(nn.Module):

    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:
        super().__init__()
        self.self_attention_block = self_attention_block
        self.cross_attention_block = cross_attention_block
        self.feed_forward_block = feed_forward_block
        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))
        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))
        x = self.residual_connections[2](x, self.feed_forward_block)
        return x
  
class Decoder(nn.Module):

    def __init__(self, features: int, layers: nn.ModuleList) -> None:
        super().__init__()
        self.layers = layers
        self.norm = LayerNormalization(features)

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        return self.norm(x)

class ProjectionLayer(nn.Module):

    def __init__(self, d_model, vocab_size) -> None:
        super().__init__()
        self.proj = nn.Linear(d_model, vocab_size)

    def forward(self, x) -> None:
        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)
        return self.proj(x)
  
class Transformer(nn.Module):

    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.src_pos = src_pos
        self.tgt_pos = tgt_pos
        self.projection_layer = projection_layer

    def encode(self, src, src_mask):
        # (batch, seq_len, d_model)
        src = self.src_embed(src)
        src = self.src_pos(src)
        return self.encoder(src, src_mask)
  
    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):
        # (batch, seq_len, d_model)
        tgt = self.tgt_embed(tgt)
        tgt = self.tgt_pos(tgt)
        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)
  
    def project(self, x):
        # (batch, seq_len, vocab_size)
        return self.projection_layer(x)
  
def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:
    # Create the embedding layers
    src_embed = InputEmbeddings(d_model, src_vocab_size)
    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)

    # Create the positional encoding layers
    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)
    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)
  
    # Create the encoder blocks
    encoder_blocks = []
    for _ in range(N):
        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)
        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)
        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)
        encoder_blocks.append(encoder_block)

    # Create the decoder blocks
    decoder_blocks = []
    for _ in range(N):
        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)
        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)
        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)
        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)
        decoder_blocks.append(decoder_block)
  
    # Create the encoder and decoder
    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))
    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))
  
    # Create the projection layer
    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)
  
    # Create the transformer
    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)
  
    # Initialize the parameters
    for p in transformer.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
  
    return transformer
```

###### [Transformer的训练流程](https://colab.research.google.com/drive/1Ho9oKrys7r3efunW5g-6dOJad8_qp1q3)

在作Transformer训练时，需要准备输入输出序列，整个输入序列会被同时提供给编码器（Encoder），而对于解码器（Decoder）而言，它会接收目标输出序列的一个偏移版本作为输入。

编码器输入：编码器接收完整的源序列（例如，一段话或句子）。在这个阶段，没有词语被特意留空。编码器的任务是处理这个输入并生成一系列高级特征表示。
解码器输入：解码器接收的是目标序列（即希望生成的输出序列）的全部内容，但是有一个重要的细节——为了训练解码器学会顺序生成每个词，目标序列通常会在时间步上向右移动一位（即加上一个开始标记，并将原始序列中的每个词都向后挪动），这样在每个时间点上解码器试图预测的目标词实际上是原目标序列中下一个位置的词。这意味着，在训练时，解码器实际上是在学习根据已经看到的所有先前目标词（包括移位后的开始标记和之前生成的所有词）来预测接下来的一个词。
训练目标：对于每个时间步骤，解码器尝试预测原始目标序列中的下一个词。损失函数通常是基于所有时间步骤上的预测误差累计计算的（比如使用交叉熵损失），这引导模型调整参数以最小化预测误差。
通过这种方式，Transformer模型可以有效地训练来执行序列到序列的任务，如翻译、文本摘要等，而不需要手动去“空出”某些词语进行预测。这种方法允许模型在一个端到端的框架下学习复杂的转换规则，从输入序列到输出序列。

针对大语言模型，输入输出的数据对需要大量的准备工作，互联网上有大量语料，但没有标注，所以

研究人员和工程师们采用了一种称为自监督学习的方法来利用大量的未标注文本数据，这种方法特别适用于语言模型的预训练阶段。以下是具体如何操作的：

自监督学习与掩码语言模型
**掩码语言模型（Masked Language Model, MLM）**：这是BERT及其变体所采用的一种策略。在训练过程中，输入文本中的某些词语会被随机掩盖（例如用特殊标记[MASK]替换），然后模型的任务是基于上下文预测这些被掩盖的词语。这样，即使没有人工标注的数据对，也可以创建一个有效的训练目标，即根据周围的词来猜测被掩盖的词是什么。
**因果语言模型（Causal Language Model, CLM）**：这是GPT系列模型采用的方式。在这种设置下，模型尝试根据给定的一段文本前面的所有词来预测下一个词。这里不掩盖任何词语，而是试图从左到右或从右到左地生成序列。这同样不需要显式的输入输出数据对，只需要连续的文本片段作为训练材料。
**去噪自编码器（Denoising Autoencoder）**：这种方法涉及向输入数据添加一些噪声（比如随机删除、交换或插入词语），然后训练模型以重建原始的无噪声版本。这也是利用大量未标注文本数据的有效方式之一。
通过上述方法，可以利用互联网上广泛存在的未标注文本数据进行模型的预训练，从而学习到丰富的语言结构和模式。预训练之后，模型可以通过微调（fine-tuning）过程应用于具体的下游任务，此时会使用到针对特定任务准备的较小规模的标注数据集。这种两阶段的方法——首先通过自监督学习进行广泛的预训练，然后针对特定任务进行调整——已经成为现代自然语言处理中非常成功的范式。

### 模型的比较

三种模型的核心都在对输入序列的信息进行全局性的语义理解，Encoder实现全局语义理解。Decoder会根据应用的类型有差别：如果只是分类识别，那么可以没有Decoder，如同典型的CNN，如果需要输出不固定的序列，那么需要Decoder。

三种模型获得全局理解的方法不同：

- RNN如同顺序阅读，只读一遍，最后综合成整体理解
- CNN多次阅读，只次阅读的层次不同，最后一次的阅读获得整体理解
- Transformer多头同时交叉阅读，最后整合成获得语义数据库

## 四、大语言模型应用

##### 提示词工程

由于大语言模型的输出是基于语义空间生成，所以语言模型参数两种语义空间：一个预训练的所有知识的语义空间，二是输入序列的语义空间，第一个语义空间是基础，第二个语义空间，即输入序列的语义空间才是输出的核心依据，因此，语言模型适合于做解释、拓展提示词的任务，因为这样仍然是在输入语义空间之内。

针对矛盾的判断，以及纠错等问题，相对会不适合，因为回答的输出已经超出了输入的语义空间，大模型不会超出输入的语义空间的范围，也没有逻辑推理能力及事实验证能力。这也是提示词应尽量避免误导及矛盾的提示信息的原因。

## 五、神对网络应用及展望
